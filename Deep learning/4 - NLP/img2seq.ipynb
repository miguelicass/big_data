{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mental-gasoline",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/img2seq.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/img2seq.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5OiQmfbX29mA",
   "metadata": {
    "id": "5OiQmfbX29mA"
   },
   "source": [
    "## Generate CAPTCHA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-lexington",
   "metadata": {
    "id": "willing-lexington"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-potential",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "virgin-potential",
    "outputId": "8d1d35dc-4b81-4272-af35-089795928694"
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "#chars.update(string.ascii_lowercase)\n",
    "#chars.update(string.ascii_uppercase)\n",
    "chars.update({str(i) for i in range(10)})\n",
    "chars = sorted(chars)\n",
    "print('Number of chars: {0}, chars: {1}'.format(len(chars), chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-employment",
   "metadata": {
    "id": "intelligent-employment"
   },
   "outputs": [],
   "source": [
    "img_dir = './captcha/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEAoZXmdn8nk",
   "metadata": {
    "id": "cEAoZXmdn8nk"
   },
   "outputs": [],
   "source": [
    "#!pip install captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-bottle",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "limited-bottle",
    "outputId": "f4abd8d8-8c96-4d85-af8f-acd214b17e3b"
   },
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "import uuid\n",
    "create_dataset = True\n",
    "captcha_len = 3\n",
    "width = 40 + 20 * captcha_len\n",
    "height = 100\n",
    "n_images = 40000\n",
    "if create_dataset:\n",
    "    image = ImageCaptcha(width = width, height = height)\n",
    "    print('Sample captcha str', np.random.choice(chars, captcha_len))\n",
    "    seen = set()\n",
    "    for _ in tqdm(range(n_images)):\n",
    "        combi = np.random.choice(chars, captcha_len)\n",
    "        captcha = ''.join(combi)\n",
    "        image.write(captcha, '{0}{1}_{2}.png'.format(img_dir, captcha, uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-worcester",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "manufactured-worcester",
    "outputId": "3ea40062-b23b-4903-cb2c-e14d96c5ebe8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of captchas', len(os.listdir(img_dir)))\n",
    "print('Some captchas', os.listdir(img_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-idaho",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "future-idaho",
    "outputId": "45ea9621-4e37-4d0c-ceae-607a1e4458d1"
   },
   "outputs": [],
   "source": [
    "## Plot first sample\n",
    "name = os.listdir(img_dir)[0]\n",
    "x = tf.keras.preprocessing.image.load_img(os.path.join(img_dir, name))\n",
    "x = tf.keras.preprocessing.image.img_to_array(x).astype(np.uint8)\n",
    "print('image shape: ', x.shape)\n",
    "plt.imshow(x)\n",
    "plt.title(name.split('_')[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BzoKTysv3P37",
   "metadata": {
    "id": "BzoKTysv3P37"
   },
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-homework",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "vertical-homework",
    "outputId": "ca8777c6-ed29-474c-e3fa-22ec22a478cf"
   },
   "outputs": [],
   "source": [
    "data_dir = img_dir\n",
    "# Get list of all the images\n",
    "images, labels = zip(*[(os.path.join(img_dir, name), name.split('_')[0])\n",
    "                       for name in os.listdir(img_dir)])\n",
    "images, labels = (np.array(list(images)), np.array(list(labels)))\n",
    "characters = sorted(set(char for label in labels for char in label))\n",
    "\n",
    "print(\"Number of images found: \", len(images))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n",
    "display(Image(images[0]))\n",
    "print('captcha:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-colors",
   "metadata": {
    "id": "rVZKjkVOpRbN"
   },
   "source": [
    "### Characters processing\n",
    "For converting the characters to one-hot encoding, we will use [tf.keras.layers.experimental.preprocessing.StringLookup\n",
    "](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup). \n",
    "```python\n",
    "tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    max_tokens=None, num_oov_indices=1, mask_token='',\n",
    "    oov_token='[UNK]', vocabulary=None, encoding=None, invert=False,\n",
    "    **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-calculator",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ultimate-calculator",
    "outputId": "5eb049d0-3a4d-40e2-a7eb-e309d3c574b5"
   },
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "train_samples = int(0.7 * len(images))\n",
    "val_split = int(0.8 * len(images))\n",
    "x_train, y_train = images[:train_samples], labels[:train_samples]\n",
    "x_val, y_val = images[train_samples:val_split], labels[train_samples:val_split]\n",
    "x_test, y_test = images[val_split:], labels[val_split:]\n",
    "print('x_train, y_train shape: ', x_train.shape, y_train.shape)\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    \n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    zeros = tf.zeros((captcha_len,1))\n",
    "    return ((img, zeros), label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-adjustment",
   "metadata": {
    "id": "valuable-infection"
   },
   "source": [
    "### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-thing",
   "metadata": {
    "id": "compatible-thing"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(encode_single_sample)\n",
    "train_dataset = train_dataset.batch(batch_size).cache().shuffle(50).prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).map(encode_single_sample)\n",
    "validation_dataset = validation_dataset.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).map(encode_single_sample)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-bibliography",
   "metadata": {
    "id": "homeless-bibliography"
   },
   "outputs": [],
   "source": [
    "# plot samples\n",
    "_, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for (images_batch, zeros_batch), labels_batch in train_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        img = (images_batch[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(labels_batch[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-freeze",
   "metadata": {},
   "source": [
    "## Questions 1: Create a model like a seq2seq\n",
    "Study the impact of `encoder_vec_dim` on the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-trade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "comparable-trade",
    "outputId": "96049b86-b3e5-4d7c-fcbb-9879038773a9"
   },
   "outputs": [],
   "source": [
    "encoder_vec_dim = ...  # dimension of the encoder vector\n",
    "\n",
    "# Encoder\n",
    "encoder_input = tf.keras.Input(shape=(width, height, 3), name='encoder_input')\n",
    "\n",
    "## Convolution + pooling layers\n",
    "...\n",
    "## Flatten()\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# encoded_vector\n",
    "x = layers.Dense(encoder_vec_dim, activation='relu')(...)\n",
    "encoded_vector = [x, x]\n",
    "\n",
    "\n",
    "\n",
    "# Decoder: encoded_vector is the input state to the first decoder RNN\n",
    "decoder_input = tf.keras.Input(shape=(captcha_len, 1), name='decoder_input')\n",
    "decoder_output = layers.LSTM(encoder_vec_dim,\n",
    "                             return_sequences=True,\n",
    "                             name=\"decoder\")(decoder_input,\n",
    "                                             initial_state=encoded_vector)\n",
    "output = layers.TimeDistributed(\n",
    "    layers.Dense(len(characters) + 1, activation='softmax'))(decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model([encoder_input, decoder_input], output)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-nothing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "magnetic-nothing",
    "outputId": "e1dfc601-d8b2-4893-8b86-c34c9f9598c1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-greek",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dense-greek",
    "outputId": "92d3385b-0a3e-4541-f336-3591bb9ed45a"
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "early_stopping_patience = 50\n",
    "# Add early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-recruitment",
   "metadata": {},
   "source": [
    "### Evaluate Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VrWcPKBPwRw_",
   "metadata": {
    "id": "VrWcPKBPwRw_"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-blond",
   "metadata": {
    "id": "psychological-blond"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for (images_batch, zeros_batch), labels_batch in test_dataset.take(1):\n",
    "    y_preds = model.predict([images_batch, zeros_batch]).argmax(-1)\n",
    "    for i in range(16):\n",
    "        img = (images_batch[i] * 255).numpy().astype(\"uint8\")\n",
    "        label = tf.strings.reduce_join(num_to_char(\n",
    "            labels_batch[i])).numpy().decode(\"utf-8\")\n",
    "        label_pred = tf.strings.reduce_join(num_to_char(\n",
    "            y_preds[i])).numpy().decode(\"utf-8\")\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4,\n",
    "           i % 4].set_title('real:{0}, pred:{1}'.format(label, label_pred))\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_model = tf.keras.applications.MobileNetV2(input_shape=(width, height, 3), include_top=False)\n",
    "#pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-vacuum",
   "metadata": {},
   "source": [
    "## Extra: Audio Captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "aud_dir = './captcha_audio/'\n",
    "if not os.path.exists(aud_dir):\n",
    "    os.makedirs(aud_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "from captcha.audio import AudioCaptcha\n",
    "\n",
    "audio = AudioCaptcha()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import wavio\n",
    "captcha_len = 3\n",
    "n_audios = 50\n",
    "audio = AudioCaptcha()\n",
    "print('Sample captcha str', np.random.choice(chars, captcha_len))\n",
    "seen = set()\n",
    "for _ in tqdm(range(n_audios)):\n",
    "    combi = np.random.choice(chars, captcha_len)\n",
    "    captcha = ''.join(combi)\n",
    "    captcha_path = '{0}{1}_{2}.wav'.format(aud_dir, captcha, uuid.uuid4())\n",
    "    audio.write(captcha, captcha_path)\n",
    "    \n",
    "    wav = wavio.read(captcha_path).data\n",
    "    # convert to 16-bits\n",
    "    max_val = 2 ** 16 - 1 \n",
    "    wav = (wav / 255.0 * max_val).astype(np.int16)\n",
    "    wav_tf = tf.audio.encode_wav(\n",
    "    wav, 8000, name=None\n",
    "    )\n",
    "    os. remove(captcha_path)\n",
    "    tf.io.write_file(\n",
    "    captcha_path, wav_tf, name=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of captchas', len(os.listdir(aud_dir)))\n",
    "print('Some captchas', os.listdir(aud_dir)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-institute",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "manufactured-worcester",
    "outputId": "3ea40062-b23b-4903-cb2c-e14d96c5ebe8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(aud_dir, os.listdir(aud_dir)[0]) \n",
    "audio_binary = tf.io.read_file(file_path)\n",
    "audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "print(audio.shape)\n",
    "waveform = tf.squeeze(audio, axis=-1)\n",
    "print(waveform.shape)\n",
    "print('captcha',os.listdir(aud_dir)[0].split('_')[0])\n",
    "display.display(display.Audio(waveform, rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all the images\n",
    "audios, labels = zip(*[(os.path.join(aud_dir, name), name.split('_')[0])\n",
    "                       for name in os.listdir(aud_dir)])\n",
    "audios, labels = (np.array(list(audios)), np.array(list(labels)))\n",
    "characters = sorted(set(char for label in labels for char in label))\n",
    "print(\"Number of audios found: \", len(audios))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-relevance",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ultimate-calculator",
    "outputId": "5eb049d0-3a4d-40e2-a7eb-e309d3c574b5"
   },
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), num_oov_indices=0, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_samples = int(0.7 * len(audios))\n",
    "val_split = int(0.8 * len(audios))\n",
    "x_train, y_train = audios[:train_samples], labels[:train_samples]\n",
    "x_val, y_val = audios[train_samples:val_split], labels[train_samples:val_split]\n",
    "x_test, y_test = audios[val_split:], labels[val_split:]\n",
    "print('x_train, y_train shape: ', x_train.shape, y_train.shape)\n",
    "\n",
    "def encode_single_sample(aud_path, label):\n",
    "    \n",
    "    audio_binary = tf.io.read_file(aud_path)\n",
    "    audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "    waveform = tf.squeeze(audio, axis=-1)[:70000]\n",
    "    \n",
    "    zero_padding = tf.zeros([70000] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "    # Concatenate audio with padding so that all audio clips will be of the\n",
    "    # same length\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    \n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    zeros = tf.zeros((captcha_len,1))\n",
    "    return ((equal_length, zeros), label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-intranet",
   "metadata": {
    "id": "valuable-infection"
   },
   "source": [
    "### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-founder",
   "metadata": {
    "id": "compatible-thing"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(encode_single_sample)\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "    \n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).map(encode_single_sample)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).map(encode_single_sample)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (audios_batch, zeros_batch), labels_batch in train_dataset.take(1):\n",
    "    for i in range(4):\n",
    "        label = tf.strings.reduce_join(num_to_char(labels_batch[i])).numpy().decode(\"utf-8\")\n",
    "        print('captcha',label)\n",
    "        display.display(display.Audio(audios_batch[i], rate=8000))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "img2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
