{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/RNN/Seq2seq.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/Seq2seq.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP8SEUBHAyTG"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we train a model to learn to add two numbers, provided as strings.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Input: \"535+61\"\n",
    "- Output: \"596\"\n",
    "\n",
    "[Notebook from Keras Tutorial](https://keras.io/examples/nlp/addition_rnn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq to seq model\n",
    "\n",
    "Keras provides the `return_state` argument to the LSTM layer that will provide access to the hidden state output (state_h) and the cell state (state_c). Note that `LSTM` has 2 state  tensors, but `GRU`\n",
    "only has one.\n",
    "\n",
    "To configure the initial state of the layer, just call the layer with additional\n",
    "keyword argument `initial_state`.\n",
    "Note that the shape of the state needs to match the unit size of the layer, like in the\n",
    "example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(timesteps, features, output_timesteps) = (10, 5, 12)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = tf.keras.Input(shape=(timesteps, features),\n",
    "                               name='encoder_input')\n",
    "\n",
    "# Return states in addition to output\n",
    "_, state_h, state_c = layers.LSTM(64, return_state=True,\n",
    "                                       name=\"encoder\")(encoder_input)\n",
    "# Enncoded vector\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input = tf.keras.Input(shape=(output_timesteps, 1),\n",
    "                               name='decoder_input')\n",
    "\n",
    "# Pass the 2 states to a new LSTM layer, as initial state\n",
    "decoder_output = layers.LSTM(64, return_sequences=True,\n",
    "                             name=\"decoder\")(decoder_input,\n",
    "                                             initial_state=encoder_state)\n",
    "output = layers.TimeDistributed(layers.Dense(5))(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, batch_sz, max_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.max_len = max_len\n",
    "\n",
    "        ##________ LSTM layer in Encoder ------- ##\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                               return_sequences=True,\n",
    "                                               return_state=True)\n",
    "\n",
    "    def call(self, encoder_input):\n",
    "        _, state_h, state_c = self.lstm_layer(encoder_input)\n",
    "        # Enncoded vector\n",
    "        encoder_state = [state_h, state_c]\n",
    "        return encoder_state\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz, max_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.max_len = max_len\n",
    "        self.decoder_input = tf.zeros((self.batch_sz, self.max_len, 1))\n",
    "\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                               return_sequences=True)\n",
    "\n",
    "    def call(self, encoder_state):\n",
    "        x = self.lstm_layer(self.decoder_input, initial_state=encoder_state)\n",
    "        output = layers.TimeDistributed(layers.Dense(5))(x)\n",
    "        return output\n",
    "    \n",
    "encoder_input = tf.keras.Input(shape=(10, 10),\n",
    "                               name='encoder_input')\n",
    "encoder = Encoder(10, 10, 5)\n",
    "encoder_state = encoder(encoder_input)\n",
    "\n",
    "decoder= Decoder(10, 10, 5)\n",
    "outputs = decoder(encoder_state)\n",
    "\n",
    "model = keras.Model(encoder_input, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use [layers.RepeatVector](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
    "model.add(layers.LSTM(128, input_shape=(timesteps, features)))\n",
    "model.add(layers.RepeatVector(output_timesteps))\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "# Apply a dense layer to the every temporal slice of an input\n",
    "model.add(layers.Dense(5, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zKygYZvAyTH"
   },
   "source": [
    "## Generate the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 3\n",
    "max_int = 10**max_digits - 1\n",
    "max_len = max_digits + 1 + max_digits\n",
    "out_max_len = len(str(max_int + max_int))\n",
    "print('max_digits : {0}, max_int: {1}, max_len: {2}, out_max_len: {3}'.format(\n",
    "    max_digits, max_int, max_len, out_max_len))\n",
    "print('max input length from {0}+{0} is {1}'.format(max_int,max_len))\n",
    "print('max sum: {0}+{0}={1}'.format(max_int,max_int+max_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN-5-VzOAyTH"
   },
   "outputs": [],
   "source": [
    "def generate_sample(max_len, max_int, out_max_len):\n",
    "    a, b = np.random.randint(max_int, size=2)\n",
    "    sentence = '{0}+{1}'.format(a, b)\n",
    "    sentence = sentence + ' ' * (max_len - len(sentence))  # padding\n",
    "    result = str(a + b)\n",
    "    result = result + ' ' * (out_max_len - len(result))  # padding\n",
    "    return sentence, result\n",
    "\n",
    "\n",
    "sentences = []\n",
    "results = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(sentences) < 50000:\n",
    "    sentence, result = generate_sample(max_len, max_int, out_max_len)\n",
    "    if sentence in seen:\n",
    "        continue\n",
    "    seen.add(sentence)\n",
    "    sentences.append(sentence)\n",
    "    results.append(result)\n",
    "print(\"Total sentences:\", len(sentences))\n",
    "print('Some examples:', list(zip(sentences[:3], results[:3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-RK8BOCAyTI"
   },
   "source": [
    "## Vectorize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"0123456789+ \"\n",
    "\n",
    "char_indices = {c:i for i, c in enumerate(sorted(chars))}\n",
    "print('char_indices', char_indices)\n",
    "indices_char = {i:c for c,i in char_indices.items()}\n",
    "print('indices_char', indices_char)\n",
    "\n",
    "def vectorize_sentence(sentence, char_indices):\n",
    "    x = np.zeros((len(sentence), len(char_indices)))\n",
    "    for i, c in enumerate(list(sentence)):\n",
    "        x[i, char_indices[c]] = 1\n",
    "    return x\n",
    "\n",
    "x = vectorize_sentence('13+11', char_indices)\n",
    "\n",
    "print('sentence: 13+11')\n",
    "print('vectorize_sentence inds:', x.argmax(-1))\n",
    "print('vectorize_sentence :', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_sentence(x, indices_char):\n",
    "    return \"\".join(indices_char[i] for i in x)\n",
    "\n",
    "def mat_to_sentence(x, indices_char):\n",
    "    x = x.argmax(axis=-1)\n",
    "    return \"\".join(indices_char[i] for i in x)\n",
    "\n",
    "mat_to_sentence(x, indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9hT0wDAAyTI"
   },
   "outputs": [],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(sentences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), out_max_len, len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x[i] = vectorize_sentence(sentence, char_indices)\n",
    "for i, sentence in enumerate(results):\n",
    "    y[i] = vectorize_sentence(sentence, char_indices)\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "val_split = int(0.8 * len(x))\n",
    "test_split = int(0.9 * len(x))\n",
    "\n",
    "(x_train, y_train) = x[:val_split], y[:val_split]\n",
    "(x_val, y_val) = x[val_split:test_split], y[val_split:test_split]\n",
    "(x_test, y_test) = x[test_split:], y[test_split:]\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqWf7pmuAyTI"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input = tf.keras.Input(\n",
    "    shape=(max_len, len(chars)), name='encoder_input')\n",
    "\n",
    "# Return states in addition to output\n",
    "_, state_h, state_c = layers.LSTM(encoded_dim, return_state=True, name=\"encoder\")(\n",
    "    encoder_input\n",
    ")\n",
    "\n",
    "# Enncoded vector\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_input = tf.keras.Input(\n",
    "    shape=(out_max_len, 1), name='decoder_input')\n",
    "\n",
    "# Pass the 2 states to a new LSTM layer, as initial state\n",
    "decoder_output = layers.LSTM(encoded_dim, return_sequences=True, name=\"decoder\")(\n",
    "    decoder_input, initial_state=encoder_state\n",
    ")\n",
    "output = layers.TimeDistributed(layers.Dense(len(chars), activation='softmax'))(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The inputs of the decoder are zeros\n",
    "decoder_input_data = np.zeros((len(x_train), out_max_len, 1))\n",
    "decoder_input_data_val = np.zeros((len(x_val), out_max_len, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "batch_size=64\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        [x_train, decoder_input_data],\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=([x_val, decoder_input_data_val], y_val),\n",
    "    )\n",
    "\n",
    "    for i in range(5):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], 1*y_val[ind] \n",
    "        preds = np.argmax(model.predict([rowx, decoder_input_data_val[[0],:]]), axis=-1).flatten()\n",
    "        q = mat_to_sentence(rowx[0], indices_char)\n",
    "        correct = mat_to_sentence(rowy, indices_char)\n",
    "        guess = vec_to_sentence(preds, indices_char)\n",
    "        print()\n",
    "        print(\"Input: \", q, \"Correct output\", correct)\n",
    "        print('Prediction')\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data_test = np.zeros((len(x_test), out_max_len, 1))\n",
    "\n",
    "results = model.evaluate([x_test, decoder_input_data_test], y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Find a model with test `accuracy> 0.9`\n",
    "\n",
    "\n",
    "Study the influence of the encoded vector dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dim = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "encoder_input = tf.keras.Input(\n",
    "    shape=(max_len, len(chars)), name='encoder_input')\n",
    "\n",
    "# Return states in addition to output\n",
    "_, state_h, state_c = layers.LSTM(encoded_dim, return_state=True, name=\"encoder\")(\n",
    "    encoder_input\n",
    ")\n",
    "\n",
    "# Enncoded vector\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_input = tf.keras.Input(\n",
    "    shape=(out_max_len, 1), name='decoder_input')\n",
    "\n",
    "decoder_output = layers.LSTM(encoded_dim, return_sequences=True, name=\"decoder\")(\n",
    "    decoder_input, initial_state=encoder_state\n",
    ")\n",
    "output = layers.TimeDistributed(layers.Dense(len(chars), activation='softmax'))(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The inputs of the decoder are zeros\n",
    "decoder_input_data = np.zeros((len(x_train), out_max_len, 1))\n",
    "decoder_input_data_val = np.zeros((len(x_val), out_max_len, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "batch_size=64\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        [x_train, decoder_input_data],\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=([x_val, decoder_input_data_val], y_val),\n",
    "    )\n",
    "\n",
    "    for i in range(5):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], 1*y_val[ind] \n",
    "        preds = np.argmax(model.predict([rowx, decoder_input_data_val[[0],:]]), axis=-1).flatten()\n",
    "        q = mat_to_sentence(rowx[0], indices_char)\n",
    "        correct = mat_to_sentence(rowy, indices_char)\n",
    "        guess = vec_to_sentence(preds, indices_char)\n",
    "        print()\n",
    "        print(\"Input: \", q, \"Correct output\", correct)\n",
    "        print('Prediction')\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data_test = np.zeros((len(x_test), out_max_len, 1))\n",
    "results = model.evaluate([x_test, decoder_input_data_test], y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Create a similar model for integer division, rounded to 3 decimals:\n",
    "```python\n",
    "'999/7' -> '142.714'\n",
    "'3/4' -> '0.75'\n",
    "'1/3' -> '0.333'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 3\n",
    "max_int = 10**max_digits - 1\n",
    "max_len = ...#\n",
    "out_max_len = ...#\n",
    "print('max_digits : {0}, max_int: {1}, max_len: {2}, out_max_len: {3}'.format(\n",
    "    max_digits, max_int, max_len, out_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(max_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(max_len, max_int, out_max_len):\n",
    "    a = np.random.randint(...)\n",
    "    b = np.random.randint(...) # zero division\n",
    "    sentence = ...\n",
    "    sentence = sentence + ' ' * (max_len - len(sentence))  # padding\n",
    "    result = str(np.round(a / b, 3))\n",
    "    result = result + ' ' * (out_max_len - len(result))  # padding\n",
    "    return sentence, result\n",
    "\n",
    "sentences = []\n",
    "results = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(sentences) < ...:\n",
    "    sentence, result = generate_sample(max_len, max_int, out_max_len)\n",
    "    if sentence in seen:\n",
    "        continue\n",
    "    seen.add(sentence)\n",
    "    sentences.append(sentence)\n",
    "    results.append(result)\n",
    "print(\"Total sentences:\", len(sentences))\n",
    "print('Some examples:', list(zip(sentences[:3], results[:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data vectorization\n",
    "\n",
    "chars = ...\n",
    "\n",
    "char_indices = {c:i for i, c in enumerate(sorted(chars))}\n",
    "print('char_indices', char_indices)\n",
    "indices_char = {i:c for c,i in char_indices.items()}\n",
    "print('indices_char', indices_char)\n",
    "\n",
    "def vectorize_sentence(sentence, char_indices):\n",
    "    x = np.zeros((len(sentence), len(char_indices)))\n",
    "    for i, c in enumerate(list(sentence)):\n",
    "        x[i, char_indices[c]] = 1\n",
    "    return x\n",
    "\n",
    "x = vectorize_sentence('13/11', char_indices)\n",
    "\n",
    "print('sentence: 13/11')\n",
    "print('vectorize_sentence inds:', x.argmax(-1))\n",
    "print('vectorize_sentence :', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_sentence(x, indices_char):\n",
    "    return \"\".join(indices_char[i] for i in x)\n",
    "\n",
    "def mat_to_sentence(x, indices_char):\n",
    "    x = x.argmax(axis=-1)\n",
    "    return \"\".join(indices_char[i] for i in x)\n",
    "\n",
    "mat_to_sentence(x, indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(sentences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), out_max_len, len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x[i] = vectorize_sentence(sentence, char_indices)\n",
    "for i, sentence in enumerate(results):\n",
    "    y[i] = vectorize_sentence(sentence, char_indices)\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input = tf.keras.Input(\n",
    "    shape=(max_len, len(chars)), name='encoder_input')\n",
    "\n",
    "...\n",
    "\n",
    "# Enncoded vector\n",
    "encoder_state = ...\n",
    "\n",
    "# Decoder\n",
    "decoder_input = tf.keras.Input(\n",
    "    shape=(out_max_len, 1), name='decoder_input')\n",
    "\n",
    "# Pass the 2 states to a new LSTM layer, as initial state\n",
    "decoder_output = ...\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "decoder_input_data = np.zeros((len(x_train), out_max_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "batch_size=64\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        [x_train, decoder_input_data],\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=([x_val, decoder_input_data_val], y_val),\n",
    "    )\n",
    "\n",
    "    for i in range(5):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], 1*y_val[ind] \n",
    "        preds = np.argmax(model.predict([rowx, decoder_input_data_val[[0],:]]), axis=-1).flatten()\n",
    "        q = mat_to_sentence(rowx[0], indices_char)\n",
    "        correct = mat_to_sentence(rowy, indices_char)\n",
    "        guess = vec_to_sentence(preds, indices_char)\n",
    "        print()\n",
    "        print(\"Input: \", q, \"Correct output\", correct)\n",
    "        print('Prediction')\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
