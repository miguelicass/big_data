{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "\n",
    "<hr>\n",
    "\n",
    "NLTK (*Natural Language Tool Kit*) es un módulo para el procesamiento de lenguaje natural. Está muy desarrollado para la lengua inglesa, y menos en español.\n",
    "\n",
    "Abordamos una introducción de los conceptos básicos,\n",
    "intentando ceñirnos al español en la medida de lo posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargamos la librería y uno de los libros que trae, text1, que es Moby Dick:\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al importar la librería se abre una ventana con una aplicacíon para descargar los paquetes que necesitemos:\n",
    "\n",
    "<img src='./figuras/nltk-descargador.png' alt='' style='height:300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga de textos\n",
    "\n",
    "Hay muchas fuentes de las que se pueden descargar textos para alnalizar. una de ellas es la propia librería *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "<Text: Moby Dick by Herman Melville 1851>\n",
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.', 'He', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'He', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '\"', 'While', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',']\n"
     ]
    }
   ],
   "source": [
    "# Descargamos un libro de ese módulo:\n",
    "\n",
    "from nltk.book import text1 # Moby Dick\n",
    "\n",
    "print(text1)\n",
    "print(text1[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una segunda fuente repleta de textos es Internet: cualquier página, en una url cualquiera, puede ser descagada para analizar su contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Áreas e intereses principales:\\r\\n                    Programación funcional, métodos formales,\\r\\n                    transformación de programas.\\r\\n                    Entornos de programación y herramientas de visualización.\\r\\n                    Testing and performance evaluation, y Cloud computing.\\r\\n\\t\\t\\t\\t\\tEnseñanza de la Informática.\\r\\n                    Historia de las matemáticas.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "archivo = urllib.request.urlopen(\"http://antares.sip.ucm.es/cpareja/\")\n",
    "con_etiquetas = archivo.read()\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "texto_limpio = BeautifulSoup(con_etiquetas, \"lxml\")\n",
    "texto = texto_limpio.get_text(strip=True)\n",
    "\n",
    "# Veamos un fragmento del texto limpio de etiquetas:\n",
    "\n",
    "texto[504:887]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos dejar de citar un tercer repositorio de libros: la librería Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "396401\n",
      "-----------------------------------------------------\n",
      "Pero después me acordé de la joya que te\r\n",
      "destinaba y que te di al llegar á tu palacio. Volví, pues, y encontré á\r\n",
      "mi mujer acostada con un esclavo negro, durmiendo en los tapices de mi\r\n",
      "cama. Los maté á los dos, y vine hacia ti, muy atormentado por el\r\n",
      "recuerdo de tal aventura. Este fué el motivo de mi primera palidez y de\r\n",
      "mi enflaquecimiento. En cuanto á la causa de haber recobrado mi buen\r\n",
      "color, dispénsame de mencionarla.»\r\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "# Descarga de un texto:\n",
    "\n",
    "url_1001_noches_part_1 = \"http://www.gutenberg.org/cache/epub/47287/pg47287.txt\"\n",
    "respuesta = request.urlopen(url_1001_noches_part_1)\n",
    "mil_y_una_noches = respuesta.read().decode('utf8')\n",
    "\n",
    "print(type(mil_y_una_noches))\n",
    "print(len(mil_y_una_noches))\n",
    "print(\"-----------------------------------------------------\")\n",
    "fragmento_1001_noches = mil_y_una_noches[40150:40582]\n",
    "print(fragmento_1001_noches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "En ocasiones, el texto descargado viene ya troceado en una lista de elementos léxicos, como es el caso de Moby Dick: palabras (como 'Moby') y símbolos (como \".\"). Cada uno de ellos es un *token*.\n",
    "\n",
    "En otras ocasiones, tenemos que hacerlo nosotros. La separación de los tokens es normalmente el primer paso para examinar un texto, ya sea de lenguaje natural o de un lenguaje formal, como son los lenguajes de programación o el lenguaje algebraico.\n",
    "\n",
    "Vemos que el text1 viene ya separado en tokens, pero si deseamos trabajar con un texto traído de cualquier otra fuente, es posible que no venga separado en tokens. Hay que *tokenizarlo*, si queremos usar este anglicismo tan extendido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pero después me acordé de la joya que te\r\n",
      "destinaba y que te di al llegar á tu palacio. Volví, pues, y encontré á\r\n",
      "mi mujer acostada con un esclavo negro, durmiendo en los tapices de mi\r\n",
      "cama. Los maté á los dos, y vine hacia ti, muy atormentado por el\r\n",
      "recuerdo de tal aventura. Este fué el motivo de mi primera palidez y de\r\n",
      "mi enflaquecimiento. En cuanto á la causa de haber recobrado mi buen\r\n",
      "color, dispénsame de mencionarla.»\r\n",
      "-----------------------------------------------------\n",
      "['Pero', 'después', 'me', 'acordé', 'de', 'la', 'joya', 'que', 'te', 'destinaba', 'y', 'que', 'te', 'di', 'al', 'llegar', 'á', 'tu', 'palacio', '.', 'Volví', ',', 'pues', ',', 'y', 'encontré', 'á', 'mi', 'mujer', 'acostada', 'con', 'un', 'esclavo', 'negro', ',', 'durmiendo', 'en', 'los', 'tapices', 'de', 'mi', 'cama', '.', 'Los', 'maté', 'á', 'los', 'dos', ',', 'y', 'vine', 'hacia', 'ti', ',', 'muy', 'atormentado', 'por', 'el', 'recuerdo', 'de', 'tal', 'aventura', '.', 'Este', 'fué', 'el', 'motivo', 'de', 'mi', 'primera', 'palidez', 'y', 'de', 'mi', 'enflaquecimiento', '.', 'En', 'cuanto', 'á', 'la', 'causa', 'de', 'haber', 'recobrado', 'mi', 'buen', 'color', ',', 'dispénsame', 'de', 'mencionarla', '.', '»']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(fragmento_1001_noches)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "fragmento_1001_noches_tokens = word_tokenize(fragmento_1001_noches)\n",
    "\n",
    "print(fragmento_1001_noches_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de separación que se necesita con frecuencia es por frases. También está disponible en la librería *nltk.tokenize*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pero después me acordé de la joya que te\r\n",
      "destinaba y que te di al llegar á tu palacio. Volví, pues, y encontré á\r\n",
      "mi mujer acostada con un esclavo negro, durmiendo en los tapices de mi\r\n",
      "cama. Los maté á los dos, y vine hacia ti, muy atormentado por el\r\n",
      "recuerdo de tal aventura. Este fué el motivo de mi primera palidez y de\r\n",
      "mi enflaquecimiento. En cuanto á la causa de haber recobrado mi buen\r\n",
      "color, dispénsame de mencionarla.»\r\n",
      "-----------------------------------------------------\n",
      "['Pero después me acordé de la joya que te\\r\\ndestinaba y que te di al llegar á tu palacio.', 'Volví, pues, y encontré á\\r\\nmi mujer acostada con un esclavo negro, durmiendo en los tapices de mi\\r\\ncama.', 'Los maté á los dos, y vine hacia ti, muy atormentado por el\\r\\nrecuerdo de tal aventura.', 'Este fué el motivo de mi primera palidez y de\\r\\nmi enflaquecimiento.', 'En cuanto á la causa de haber recobrado mi buen\\r\\ncolor, dispénsame de mencionarla.»']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(fragmento_1001_noches)\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "fragmento_1001_noches_frases = sent_tokenize(fragmento_1001_noches)\n",
    "\n",
    "print(fragmento_1001_noches_frases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords, palabras vacías o palabras comunes\n",
    "\n",
    "Las *stopwords* son palabras que no aportan significado por sí solas. Típicamente, se trata de los artículos, preposiciones, conjunciones y pronombres, aunque también algunos verbos.\n",
    "\n",
    "Normalmente, estas palabras se descartan en muchas aplicaciones de análisis de lenguaje natural, pues las aplicaciones más frecuentes extraen información de las palabras con significado intrínseco, tales como sutantivos y adjetivos.\n",
    "\n",
    "Veamos cuáles son las palabras vacías en español que vienen ya definidas en la librería *nltk*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.util.LazyCorpusLoader'>\n",
      "313\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(type(stopwords))\n",
    "\n",
    "stop_espannol = stopwords.words('spanish')\n",
    "print(len(stop_espannol))\n",
    "print(stop_espannol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza: sin stopwords\n",
    "\n",
    "En el análisis de texos, normalmente no se necesitan las stopwords: estorban. Por eso, es frecuente una operación de limpieza como la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pero', 'después', 'acordé', 'joya', 'destinaba', 'di', 'llegar', 'á', 'palacio', '.', 'Volví', ',', 'pues', ',', 'encontré', 'á', 'mujer', 'acostada', 'esclavo', 'negro', ',', 'durmiendo', 'tapices', 'cama', '.', 'Los', 'maté', 'á', 'dos', ',', 'vine', 'hacia', ',', 'atormentado', 'recuerdo', 'tal', 'aventura', '.', 'Este', 'fué', 'motivo', 'primera', 'palidez', 'enflaquecimiento', '.', 'En', 'cuanto', 'á', 'causa', 'haber', 'recobrado', 'buen', 'color', ',', 'dispénsame', 'mencionarla', '.', '»']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def removeStopwords(palabras):\n",
    "     return [word for word in palabras if word not in stopwords.words('spanish')]\n",
    "\n",
    "fragmento_2001_sin_stop = removeStopwords(fragmento_1001_noches_tokens)\n",
    "\n",
    "print(fragmento_2001_sin_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de palabras significativas\n",
    "\n",
    "Y ahora se puede hacer algún análisis. La librería *nltk* permite contar la frecuencia de las palabras de un texto. \n",
    "\n",
    "Poca gracia tendría esto son un fragmento breve, así que lo hacemos con el texto completo de las 1001 noches, aunque ya te advierto que el proceso llevará un tiempo... \n",
    "\n",
    "Y mostramos los 25 tokens más repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", -> 5198\n",
      ". -> 2201\n",
      "á -> 1446\n",
      ": -> 1083\n",
      "Y -> 949\n",
      "« -> 910\n",
      "! -> 900\n",
      "» -> 897\n",
      "dijo -> 403\n",
      "rey -> 321\n",
      "Entonces -> 281\n",
      "? -> 235\n",
      "joven -> 198\n",
      "Alah -> 196\n",
      "_ -> 186\n",
      "Pero -> 181\n",
      "[ -> 177\n",
      "] -> 177\n",
      "; -> 176\n",
      "¡Oh -> 164\n",
      "the -> 162\n",
      "-- -> 162\n",
      "El -> 138\n",
      "¡oh -> 127\n",
      "efrit -> 125\n"
     ]
    }
   ],
   "source": [
    "mil_y_una_noches_tokens = word_tokenize(mil_y_una_noches)\n",
    "mil_y_una_noches_tokens_sin_stop = removeStopwords(mil_y_una_noches_tokens)\n",
    "frecuencias_terminos_1001_noches = nltk.FreqDist(mil_y_una_noches_tokens_sin_stop)\n",
    "terms_frecs = list(frecuencias_terminos_1001_noches.items())\n",
    "terms_frecs.sort(key=lambda par: par[1], reverse=True)\n",
    "\n",
    "for termino, frec in terms_frecs[:25]:\n",
    "    print(termino + \" -> \" + str(frec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es evidente que se ha de limpiar más, eliminando tokens que no tienen letras, eliminando los caracteres no letras al inicio y al final... pero esto no es importante en este momento.\n",
    "\n",
    "Para ver el resultado de nuestro análisis, no hay nada mejor que un gráfico. Para el mismo, también nos quedamos con los 25 términos más repetidos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "abcisas = [x for (x,_y) in terms_frecs[:25]]\n",
    "valores = [y for (_, y) in terms_frecs[:25]]\n",
    "\n",
    "x_pos = np.arange(len(abcisas))\n",
    "\n",
    "plt.plot(x_pos, valores)\n",
    "plt.xticks(x_pos, abcisas, rotation=60)\n",
    "\n",
    "plt.ylabel('Frecuencias')\n",
    "plt.xlabel('Términos o tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinónimos y antónimos\n",
    "\n",
    "Dentro de la librería *nltk*, el módulo *wordnet* incluye un diccionario de sinónimos y antónimos. \n",
    "Al instalar nltk, uno de los paquetes que se podía incluir era *wordnet*; si en su momento no lo instalaste, ahora es el momento.\n",
    "\n",
    "El problema es que a fecha de hoy, no existe este módulo para la lengua española :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('large.a.01'), Synset('big.s.02'), Synset('bad.s.02'), Synset('big.s.04'), Synset('big.s.05'), Synset('big.s.06'), Synset('boastful.s.01'), Synset('big.s.08'), Synset('adult.s.01'), Synset('big.s.10'), Synset('big.s.11'), Synset('big.s.12'), Synset('big.s.13'), Synset('big.r.01'), Synset('boastfully.r.01'), Synset('big.r.03'), Synset('big.r.04')]\n",
      "above average in size or number or quantity or magnitude or extent\n",
      "['a large city', 'set out for the big city', 'a large sum', 'a big (or large) barn', 'a large family', 'big businesses', 'a big expenditure', 'a large number of newspapers', 'a big group of scientists', 'large areas of the world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "sinonimos = wordnet.synsets(\"big\")\n",
    "\n",
    "print(sinonimos)\n",
    "\n",
    "print(sinonimos[0].definition())\n",
    "print(sinonimos[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'big', 'big', 'bad', 'big', 'big', 'big', 'large', 'prominent', 'big', 'heavy', 'boastful', 'braggart', 'bragging', 'braggy', 'big', 'cock-a-hoop', 'crowing', 'self-aggrandizing', 'self-aggrandising', 'big', 'swelled', 'vainglorious', 'adult', 'big', 'full-grown', 'fully_grown', 'grown', 'grownup', 'big', 'big', 'large', 'magnanimous', 'big', 'bighearted', 'bounteous', 'bountiful', 'freehanded', 'handsome', 'giving', 'liberal', 'openhanded', 'big', 'enceinte', 'expectant', 'gravid', 'great', 'large', 'heavy', 'with_child', 'big', 'boastfully', 'vauntingly', 'big', 'large', 'big', 'big']\n",
      "['small', 'little', 'small']\n"
     ]
    }
   ],
   "source": [
    "sinonimos, antonimos = [], []\n",
    "\n",
    "def sinonimos_de(palabra):\n",
    "    sinonimos = [lemma.name() for\n",
    "                 sin in wordnet.synsets(palabra)\n",
    "                 for lemma in sin.lemmas()]\n",
    "    return sinonimos\n",
    "\n",
    "def antonimos_de(palabra):\n",
    "    antonimos = [lemma.antonyms()[0].name() \n",
    "                 for sin in wordnet.synsets(palabra)\n",
    "                 for lemma in sin.lemmas() if lemma.antonyms()]\n",
    "    return antonimos\n",
    "\n",
    "print(sinonimos_de(\"big\"))\n",
    "print(antonimos_de(\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivación regresiva, o *steming*\n",
    "\n",
    "Las palabras *working*, *works* y *worked* tienen la misma raíz, y es conveniente considerarlas como en la misma categoría semántica a efectos de análisis de texto. La derivación regresiva (*stem*, en inglés) nos da la raíz de una palabra, y el algoritmo más empleado para ello, en el idioma inglés, es el de Porter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'))\n",
    "print(stemmer.stem('worked'))\n",
    "print(stemmer.stem('works'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivación regresiva de palabras no inglesas hay que acudir a otros paquetes, como\n",
    "*SnowballStemmer*, que contiene 13 idiomas además del inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trabaj\n",
      "trabaj\n",
      "trabaj\n",
      "trabaj\n",
      "trabaj\n",
      "trabaj\n",
      "trabaj\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "raiz_espannola = SnowballStemmer(\"spanish\")\n",
    "\n",
    "print(raiz_espannola.stem('trabajaba'))\n",
    "print(raiz_espannola.stem('trabajos'))\n",
    "print(raiz_espannola.stem('trabajoso'))\n",
    "print(raiz_espannola.stem('trabajaré'))\n",
    "print(raiz_espannola.stem('trabajar'))\n",
    "print(raiz_espannola.stem('trabajando'))\n",
    "print(raiz_espannola.stem('trabajaríamos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "\n",
    "+    https://likegeeks.com/es/tutorial-de-nlp-con-python-nltk/\n",
    "+    https://www.nltk.org/book/ch01.html\n",
    "+    https://pmoracho.github.io/blog/2017/01/04/NLTK-mi-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
