{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# MLlib hands-on: RandomForest para predecir la severidad del retraso"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset est\u00e1 compuesto por las siguientes variables:\n\n1. **Year** 2008\n2. **Month** 1\n3. **DayofMonth** 1-31\n4. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n5. **DepTime** hora real de salida (local, hhmm)\n6. **CRSDepTime** hora prevista de salida (local, hhmm)\n7. **ArrTime** hora real de llegada (local, hhmm)\n8. **CRSArrTime** hora prevista de llegada (local, hhmm)\n9. **UniqueCarrier** c\u00f3digo del aparato\n10. **FlightNum** n\u00famero de vuelo\n11. **TailNum** identificador de cola: aircraft registration, unique aircraft identifier\n12. **ActualElapsedTime** tiempo real invertido en el vuelo\n13. **CRSElapsedTime** en minutos\n14. **AirTime** en minutos\n15. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n16. **DepDelay** retraso a la salida, en minutos\n17. **Origin** c\u00f3digo IATA del aeropuerto de origen\n18. **Dest** c\u00f3digo IATA del aeropuerto de destino\n19. **Distance** en millas\n20. **TaxiIn** taxi in time, in minutes\n21. **TaxiOut** taxi out time in minutes\n22. **Cancelled** *si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n23. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n24. **Diverted** *si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n25. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n26. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n27. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n28. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n29. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "markdown", "metadata": {}, "source": "**IMPORTANTE:** Vamos a emplear una versi\u00f3n reducida del dataset de vuelos para entender la forma de trabajar con MLlib, y en particular, los mecanimos de exploraci\u00f3n de combinaciones de h\u00edper-par\u00e1metros para encontrar el mejor modelo, los cuales requieren ajustar varios modelos distintos varias veces. Este proceso ser\u00eda demasiado largo si utiliz\u00e1semos el dataset original de 300 MB. \n\n<p>En la siguiente celda, descargaremos este dataset reducido de Internet mediante la ejecuci\u00f3n del comandos de Linux `wget` y lo subiremos a HDFS  con el comando de HDFS `copyFromLocal`. Los datos guardados en HDFS son vol\u00e1tiles y desaparecer\u00e1n cuando el cluster sea desmantelado.</p>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Vamos a utilizar una versi\u00f3n reducida del dataset original para poder ver en funcionamiento el ajuste de hiper-par\u00e1metros, que requiere ajustar varios modelos"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Descargamos el CSV reducido y lo subimos a HDFS (esta vez no es Google Cloud Storage)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!wget https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flights_jan08.csv\n!hdfs dfs -copyFromLocal flights_jan08.csv /tmp"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Ahora lo leemos desde la ruta /tmp/flights_jan08.csv de HDFS"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Leemos el dataset de HDFS. Esta operaci\u00f3n todav\u00eda no hace la lectura hace una pasada \n# sobre los datos para inferir el esquema\nflightsDF = spark.read.option(\"header\", \"true\")\\\n                      .option(\"inferSchema\", \"true\")\\\n                      .csv(\"/tmp/flights_jan08.csv\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a utilizar algunos transformadores habituales, y un algoritmo RandomForest que es un estimador. Utilizaremos:\n\n* StringIndexer para convertir variables tipo String en variables categ\u00f3ricas pero cuyos valores son n\u00fameros reales con la parte decimal a 0, tal como necesitan los algoritmos de Spark.\n* Bucketizer para discretizar la columna de ArrDelay sin dar nombre a las categor\u00edas, solo numeros. Ser\u00e1 nuestra variable target.\n* VectorAssembler para unir las columnas de las features en una sola de tipo vector\n* RandomForest que es un estimador, como algoritmo de predicci\u00f3n de la severidad\n* Pipeline, que es un estimador y que incluir\u00e1 todos los elementos anteriores."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\n\ncleanFlightsDF = flightsDF.where(\"ArrDelay != 'NA' and DepDelay != 'NA' and DepTime != 'NA' and ArrTime != 'NA'\")\\\n                          .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(IntegerType()))\\\n                          .withColumn(\"DepDelay\", F.col(\"DepDelay\").cast(IntegerType()))\\\n                          .withColumn(\"ArrTime\", F.col(\"ArrTime\").cast(IntegerType()))\\\n                          .withColumn(\"DepTime\", F.col(\"DepTime\").cast(IntegerType()))\n\n# Definimos tres categor\u00edas: <15, entre 15 y 60, >60. Cada una se codifica con un n\u00famero real: 0.0, 1.0, 2.0 \nsplitsDelays = [-float(\"inf\"), 15, 60, float(\"inf\")]\narrDelayBucketizer = Bucketizer(splits=splitsDelays, inputCol=\"ArrDelay\", outputCol=\"ArrDelayBucketed\")\n\n# Definimos varias franjas: 00:00 - 06:00, 06:00 - 12:00, 12:00 - 18:00, 18:00 - 22:00, 22:00 - 00:00\nsplitsDepTime = [-1, 600, 1200, 1800, 2200, 2500]\ndepTimeBucketizer = Bucketizer(splits=splitsDepTime, inputCol=\"DepTime\", outputCol=\"DepTimeBucketed\")\n\n# Dividimos en train y test de manera aleatoria usando la semilla 123 para los n\u00fameros aleatorios, para que sea reproducible. \n# Esto devuelve una lista de dos DataFrames. La divisi\u00f3n har\u00e1 que el primer elemento de la lista sea un DF \n# con aprox. el 70 % de las filas. Lo usaremos para entrenar. El otro DF tendr\u00e1 aprox el 30 % restante de las filas. \nsplits = cleanFlightsDF.randomSplit([0.7, 0.3], seed = 123)\ntrainDF = splits[0].cache() # El primer DF tiene el 70 % de los datos\ntestDF = splits[1].cache()\n\n# Si quisiera generar la lista con todos los StringIndexer para TODAS las columnas:\n# categoricas = [\"asd\u00f1fkj\", \"a\u00f1dslkjadf\"]\n# indexerList = [StringIndexer(inputCol=c, outputCol=c + \"indexed\", handleInvalid=\"skip\") for c in categoricas]\n\n# Indexamos las columnas categ\u00f3ricas Origin, Dest y DayOfWeek, para traducirlas a reales con la parte decimal a 0.\n# Recordemos que esto tambi\u00e9n introduce metadatos adicionales en la columna resultante para indicar que, aunque sea\n# una columna de n\u00fameros reales, en realidad est\u00e1n representando categor\u00edas y debe ser tratada como tal por el algoritmo\noriginIndexer = StringIndexer(inputCol = \"Origin\", outputCol=\"OriginIndexed\", handleInvalid=\"skip\")\ndestIndexer = StringIndexer(inputCol = \"Dest\", outputCol=\"DestIndexed\", handleInvalid=\"skip\")\ndowIndexer = StringIndexer(inputCol = \"DayOfWeek\", outputCol=\"DayOfWeekIndexed\", handleInvalid=\"skip\")\n\n# Ahora unimos todas las columnas que se usar\u00e1n como variables en una sola columna de tipo vector llamada featuresVector\nvectorAssembler = VectorAssembler(inputCols = [\"DepDelay\", \"DepTimeBucketed\", \"OriginIndexed\", \"DestIndexed\", \"DayOfWeekIndexed\"], \n                                  outputCol = \"featuresVector\")\n\nrandomForest = RandomForestClassifier(featuresCol = \"featuresVector\",\n                                      labelCol = \"ArrDelayBucketed\",\n                                      numTrees = 50, maxBins=100)\n\npipeline = Pipeline(stages=[arrDelayBucketizer, depTimeBucketizer, dowIndexer, originIndexer, destIndexer, \n                            vectorAssembler, randomForest])"}, {"cell_type": "markdown", "metadata": {}, "source": "Aplicamos fit para ajustar el pipeline completo. Lo que esto hace es, para cada etapa:\n- Si la etapa es un Transformer, invoca al m\u00e9todo transform() pas\u00e1ndole el DF que recibe de la etapa anterior,\n  y env\u00eda el resultado (DF transformado) a la etapa siguiente del pipeline.\n- Si la etapa es un Estimator, invoca al m\u00e9todo fit() pas\u00e1ndole como argumento el DF recibido de la etapa anterior,\n  y despu\u00e9s invoca transform() sobre el objeto devuelto por fit, pasando de nuevo dicho DF como argumento. \n  Finalmente el DF devuelvo por transform es enviado a la etapa siguiente del pipeline."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "pipelineModel = pipeline.fit(trainDF)"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a hacer predicciones sobre los datos de test. Esto devolver\u00e1 un nuevo DF al que se le han a\u00f1adido varias columnas al final:\n1. **`rawPrediction`**: magnitud que tiene una interpretaci\u00f3n diferente seg\u00fan el algoritmo pero que intuitivamente nos da una\n  medida de la confianza en cada posible clase (cuanto mayor, m\u00e1s confianza se tiene en que esa es la clase del ejemplo). En\n  nuestro caso ser\u00e1 un vector de 3 n\u00fameros reales puesto que nuestro problema tiene 3 clases\n2. **`probability`**: vector de probabilidades de que el ejemplo pertenezca a cada una de las 3 clases de nuestro problema\n3. **`prediction`**: clase para la cual la rawProbability es mayor."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "predictionsDF = pipelineModel.transform(testDF)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print(\"Hay {0} ejemplos de entrenamiento\".format(trainDF.count()))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "predictionsDF.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Evaluaci\u00f3n del modelo (evaluamos las predicciones que hemos hecho sobre el DF de test)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Select (prediction, true label) and compute test error\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Objeto evaluador, para evaluar las predicciones. Compara una columna de predicciones con la columna target del verdadero valor\n# Hay varias m\u00e9tricas posibles, pero nosotros hemos elegido la m\u00e1s simple: porcentaje de acierto en clasificaci\u00f3n.\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"ArrDelayBucketed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n\naccuracy = evaluator.evaluate(predictionsDF)\n\nprint(\"Test Error = %g \" % (1.0 - accuracy))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ajuste de h\u00edper-par\u00e1metros utilizando Cross Validation sobre el subconjunto de train"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n\n# Objeto ParamGrid al que le tenemos que indicar cada uno de los par\u00e1metros sobre los que queremos buscar,\n# y la lista de valores posibles que queremos probar con cada uno. Deben ser par\u00e1metros tomados directamente\n# del objeto estimador que se a\u00f1adi\u00f3 al pipeline (no pueden ser de ning\u00fan otro modelo). En nuestro caso este objeto\n# est\u00e1 almacenado en la variable randomForest que hab\u00edamos creado en celdas anteriores.\nparamGrid = ParamGridBuilder()\\\n    .addGrid(randomForest.numTrees, [50, 100, 150])\\\n    .addGrid(randomForest.maxDepth, [3, 4, 5])\\\n    .build() # como tenemos 2 par\u00e1metros con 3 valores posibles cada uno, hay 9 combinaciones posibles\n\n# CrossValidator es un estimador. Cuando invocamos a fit(), probar\u00e1 todas las combinaciones de valores de los \n# par\u00e1metros, invocando con cada combinaci\u00f3n al m\u00e9todo fit() del objeto pipeline que le hemos pasado como argumento\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)\n\n# Como hemos hecho 3 folds, habr\u00e1 que entrenar 3 veces cada modelo candidato (cada combinaci\u00f3n de par\u00e1metros)\n# y obtener su media de accuracy. En total vamos a entrenar 27 modelos\n\ncvModel = crossval.fit(trainDF)\ncvModel.bestModel"}, {"cell_type": "markdown", "metadata": {}, "source": "#### El objeto RandomForestModel (modelo ajustado presente en la \u00faltima etapa del pipeline ya ajustado) dispone de ciertos atajos para recuperar valores de algunos par\u00e1metros (por ejemplo getNumTrees) pero no de otros (por ejemplo, no existe getMaxDepth)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "rf = cvModel.bestModel.stages[6]\nprint(\"N\u00famero \u00f3ptimo de \u00e1rboles: {0}\".format(rf.getNumTrees))\nprint(\"Max depth \u00f3ptimo: {0}\".format(rf.getOrDefault('maxDepth')))"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}