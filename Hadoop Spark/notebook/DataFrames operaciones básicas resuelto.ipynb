{"nbformat_minor": 2, "cells": [{"source": "# Operaciones b\u00e1sicas con DataFrames", "cell_type": "markdown", "metadata": {}}, {"source": "## Descripci\u00f3n de las variables", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "markdown", "metadata": {}}, {"source": "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> est\u00e1 compuesto por las siguientes variables referidas siempre al a\u00f1o 2018:\n\n1. **Month** 1-4\n2. **DayofMonth** 1-31\n3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n4. **FlightDate** fecha del vuelo\n5. **Origin** c\u00f3digo IATA del aeropuerto de origen\n6. **OriginCity** ciudad donde est\u00e1 el aeropuerto de origen\n7. **Dest** c\u00f3digo IATA del aeropuerto de destino\n8. **DestCity** ciudad donde est\u00e1 el aeropuerto de destino  \n9. **DepTime** hora real de salida (local, hhmm)\n10. **DepDelay** retraso a la salida, en minutos\n11. **ArrTime** hora real de llegada (local, hhmm)\n12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n13. **Cancelled** si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n14. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n15. **Diverted** si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n16. **ActualElapsedTime** tiempo real invertido en el vuelo\n17. **AirTime** en minutos\n18. **Distance** en millas\n19. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n20. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n22. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n23. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista.", "cell_type": "markdown", "metadata": {}}, {"source": "Leemos el fichero CSV utilizando el delimitador por defecto de Spark (\",\"). La primera l\u00ednea contiene encabezados (nombres de columnas) por lo que no es parte de los datos y debemos indicarlo con la opci\u00f3n header.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# Esto no hace nada: la lectura es lazy as\u00ed que no se lee en realidad hasta que ejecutemos una acci\u00f3n sobre flightsDF\n# Solamente se comprueba que exista y se leen los nombres de columnas\nflightsDF = spark.read.option(\"header\", \"true\")\\\n                 .csv(\"gs://ucmbucket/data/flights-jan-apr-2018.csv\")", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# Veamos el esquema (nombre y tipo de dato de cada columna). Esto son solamente metadatos, por lo que no es ninguna acci\u00f3n.\nflightsDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Month: string (nullable = true)\n |-- DayofMonth: string (nullable = true)\n |-- DayOfWeek: string (nullable = true)\n |-- FlightDate: string (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- DepDelay: string (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: string (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: string (nullable = true)\n |-- ActualElapsedTime: string (nullable = true)\n |-- AirTime: string (nullable = true)\n |-- Distance: string (nullable = true)\n |-- CarrierDelay: string (nullable = true)\n |-- WeatherDelay: string (nullable = true)\n |-- NASDelay: string (nullable = true)\n |-- SecurityDelay: string (nullable = true)\n |-- LateAircraftDelay: string (nullable = true)\n\n"}], "metadata": {}}, {"source": "Todas las columnas son cadenas de caracteres porque no hemos indicado el tipo de dato para cada columna ni tampoco le hemos pedido a Spark que intente inferirlo a partir de los datos. No queremos que todas sean string porque hay algunas num\u00e9ricas que deber\u00edan ser tratadas como tales. Vamos a intentar inferir el esquema. Esto supone una lectura un poco m\u00e1s lenta y tambi\u00e9n es m\u00e1s lento que una tercera opci\u00f3n que consiste en indicar expl\u00edcitamente el esquema para los datos en el momento de la lectura, que es la opci\u00f3n recomendada si sabemos de antemano qu\u00e9 tipo va a tener cada columna. Si lo hici\u00e9semos de esa manera, en caso de que no se pueda leer con ese esquema obtendr\u00edamos un error.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "from pyspark.sql import functions as F\n\nflightsDF = spark.read\\\n                 .option(\"header\", \"true\")\\\n                 .option(\"inferSchema\", \"true\")\\\n                 .csv(\"gs://ucmbucket/data/flights-jan-apr-2018.csv\")\n\n# Ensuciamos a prop\u00f3sito la variable ArrDelay para que pase a ser un string como suele pasar con frecuencia\nflightsDF = flightsDF.withColumn(\"ArrDelay\",\\\n                                 F.when(F.rand() < 0.1, \"NA\").otherwise(F.col(\"ArrDelay\")))\n\nflightsDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n\n"}], "metadata": {}}, {"source": "Ahora tiene mejor pinta, aunque todav\u00eda hay algunas columnas cuyo tipo de dato sigue siendo string cuando la intuici\u00f3n nos dice que deber\u00edan ser enteros.", "cell_type": "markdown", "metadata": {}}, {"source": "## Operaciones b\u00e1sicas con Data Frames", "cell_type": "markdown", "metadata": {}}, {"source": "### Contamos el n\u00famero de filas", "cell_type": "markdown", "metadata": {}}, {"source": "Es una de las primeras cosas que nos preguntamos sobre un dataset: n\u00famero de filas y columnas (cu\u00e1ntos ejemplos y cu\u00e1ntas variables tenemos para describirlos). Puesto que vamos a llevar a cabo varias transformaciones al DataFrame `flightsDF` a partir de este punto, vamos a usar `cache`() para que Spark lo mantenga en memoria en lugar de liberar la memoria ocupada tras cada acci\u00f3n.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "flightsDF.columns", "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "['Month',\n 'DayofMonth',\n 'DayOfWeek',\n 'FlightDate',\n 'Origin',\n 'OriginCity',\n 'Dest',\n 'DestCity',\n 'DepTime',\n 'DepDelay',\n 'ArrTime',\n 'ArrDelay',\n 'Cancelled',\n 'CancellationCode',\n 'Diverted',\n 'ActualElapsedTime',\n 'AirTime',\n 'Distance',\n 'CarrierDelay',\n 'WeatherDelay',\n 'NASDelay',\n 'SecurityDelay',\n 'LateAircraftDelay']"}, "metadata": {}}], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "# Extraemos los nombres de columna. Esto son solo metadatos de DataFrame, y est\u00e1n en el driver. No es necesaria ninguna\n# operaci\u00f3n sobre el cluster para recuperar la variable interna columns de cualquier DataFrame\nprint(\"Los datos tienen {0} columnas\".format(len(flightsDF.columns)))\n\nflightsDF.cache()        # Esta l\u00ednea no hace c\u00e1lculos, pero Spark anota que debe mantener este DF en memoria tras la primera vez que sea materializado\nrows = flightsDF.count() # Esto es una acci\u00f3n que obligar\u00e1 a que flightsDF sea materializado. Para ello, habr\u00e1 que llevar a cabo\n                         # las transformaciones que lo generan en la celda anterior: read y withColumn, que est\u00e1n pendientes\n\nprint(\"Los datos tienen {0} filas\".format(rows))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Los datos tienen 23 columnas\nLos datos tienen 2503113 filas\n"}], "metadata": {}}, {"source": "### Seleccionar columnas por nombre", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "flightsDF.select(\"Month\", \"DayofMonth\", \"ArrTime\")\\\n         .show() # los nombres son sensibles a may\u00fasculas", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+----------+-------+\n|Month|DayofMonth|ArrTime|\n+-----+----------+-------+\n|    1|        14|   null|\n|    1|         3|   1506|\n|    1|         6|   1543|\n|    1|         7|   1455|\n|    1|         8|   1509|\n|    1|         9|   1504|\n|    1|        10|   1455|\n|    1|        11|   1452|\n|    1|        12|   1748|\n|    1|        13|   1514|\n|    1|        15|   1456|\n|    1|        16|   1511|\n|    1|        17|   1622|\n|    1|        18|   1509|\n|    1|        19|   1449|\n|    1|        20|   1533|\n|    1|        21|   1508|\n|    1|        22|   1504|\n|    1|        23|   1616|\n|    1|        24|   1515|\n+-----+----------+-------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"source": "### Filtramos (retenemos) filas en base a los valores de una o varias columnas", "cell_type": "markdown", "metadata": {}}, {"source": "La mayor\u00eda de las transformaciones est\u00e1n definidas en el paquete `pyspark.sql.functions`, por lo que es frecuente importar el paquete completo con un alias, como `F`, en lugar de importar cada funci\u00f3n individual. A partir de ese momento usamos `F.` antes del nombre de cada funci\u00f3n, para decirle a python d\u00f3nde buscar esa funci\u00f3n.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "import pyspark.sql.functions as F  \n\n# La funci\u00f3n col se utiliza para decir que nos estamos refiriendo a la columna cuyo nombre se pasa como argumento\n\nflightsJanuary20 = flightsDF\\\n                      .where((F.col(\"DayofMonth\") == 20) & (F.col(\"Month\") == 1))\\\n                      .select(\"Month\", \"ArrTime\") # esto no desencadena ninguna operaci\u00f3n porque Spark es perezoso\n\n# C\u00faantos vuelos hay el 20 de enero de 2008\n# count() es una acci\u00f3n, as\u00ed que lanza el c\u00e1lculo. Si no hubi\u00e9ramos cacheado el DF, leer\u00eda de nuevo el CSV\nrowsJanuary20 = flightsJanuary20.count()\n\nprint(\"Hubo\", rowsJanuary20, \"vuelos el 20 de enero de 2018\")\n\n# Esto es otra acci\u00f3n aplicada sobre el DataFrame flightsJanuary. Como flightsJanuary NO est\u00e1 cacheado,\n# por tanto las operaciones \"where\" y \"select\" se ejecutan DE NUEVO para poder hacer el show()\nflightsJanuary20.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('Hubo', 16176, 'vuelos el 20 de enero de 2018')\n+-----+-------+\n|Month|ArrTime|\n+-----+-------+\n|    1|   1533|\n|    1|   1734|\n|    1|   2025|\n+-----+-------+\nonly showing top 3 rows\n\n"}], "metadata": {}}, {"source": "**No olvidemos cachear el DataFrame cuando tengamos previsto hacer varias operaciones sobre \u00e9l, o de lo contrario estaremos repitiendo muchas veces los c\u00e1lculos previos que llevaron a ese DataFrame!!**", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "# Tambi\u00e9n podemos indicar el filtrado como un string con un trozo de c\u00f3digo SQL\n# Recordemos que filter() es un alias para where() y ambas hacen exactamente lo mismo\nflightsJanuary31 = flightsDF.filter(\"DayofMonth = 20 and Month = 1\")\n\nflightsJanuary31.count()", "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "16176"}, "metadata": {}}], "metadata": {}}, {"source": "<div class=\"alert alert-block alert-info\">\n<b>RECUERDA:</b> Spark hace todas estas operaciones de manera distribuida en el cluster, por tanto cada nodo del cluster est\u00e1 filtrando filas de entre aquellas que est\u00e1n presentes en ese nodo (m\u00e1s precisamente, en ese executor). Cada executor env\u00eda al driver el recuento de cu\u00e1ntas filas ha filtrado, y los resultados son agregados en el driver para mostrar el recuento total.\n</div>", "cell_type": "markdown", "metadata": {}}, {"source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: muestra por pantalla el retraso en la llegada, el aeropuerto de origen y de destino de aquellos vuelos que tuvieron lugar en Domingo y con un retraso a la llegada mayor de 15 minutos. Muestra el esquema de dicho DataFrame resultante.\n\n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "from pyspark.sql import functions as F\n\nflightsSundayDF = flightsDF.filter(F.col(\"DayOfWeek\")==7)\\\n                           .where(F.col(\"ArrDelay\")>15)\\\n                           .select(\"ArrDelay\",\"Origin\",\"Dest\")\nflightsSundayDF.show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+------+----+\n|ArrDelay|Origin|Dest|\n+--------+------+----+\n|    21.0|   BOS| BUF|\n|    22.0|   LGA| MSN|\n|   136.0|   BOS| JFK|\n|   119.0|   DTW| JFK|\n|   687.0|   DTW| MBS|\n|    90.0|   CVG| DTW|\n|    61.0|   GNV| ATL|\n|    48.0|   ATL| SHV|\n|    45.0|   SHV| ATL|\n|    17.0|   LAS| FLL|\n+--------+------+----+\nonly showing top 10 rows\n\n"}], "metadata": {}}, {"source": "### Seleccionar filas \u00fanicas", "cell_type": "markdown", "metadata": {}}, {"source": "Para hacerse mejor idea de c\u00f3mo es una variable categ\u00f3rica, es l\u00f3gico querer cu\u00e1ntos valores distintos existen en nuestro dataset. Si consideramos todas las columnas, ser\u00eda raro tener dos filas exactamente iguales en todos los valores, pero si seleccionamos solamente una o unas pocas columnas, podemos ver cu\u00e1ntas combinaciones distintas de valores de esas columnas se dan en el dataset.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "flightsDF.dropDuplicates([\"Origin\"]).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+----------+---------+-------------------+------+--------------------+----+---------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n|Month|DayofMonth|DayOfWeek|         FlightDate|Origin|          OriginCity|Dest|       DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+-----+----------+---------+-------------------+------+--------------------+----+---------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n|    1|         2|        2|2018-01-02 00:00:00|   BGM|      Binghamton, NY| DTW|    Detroit, MI|    552|    -9.0|    747|      NA|      0.0|            null|     0.0|            115.0|   69.0|   378.0|        null|        null|    null|         null|             null|\n|    1|         2|        2|2018-01-02 00:00:00|   INL|International Fal...| MSP|Minneapolis, MN|   1038|    -9.0|   1146|   -18.0|      0.0|            null|     0.0|             68.0|   46.0|   255.0|        null|        null|    null|         null|             null|\n|    1|         1|        1|2018-01-01 00:00:00|   PSE|           Ponce, PR| MCO|    Orlando, FL|    258|    -9.0|    453|      NA|      0.0|            null|     0.0|            175.0|  161.0|  1179.0|        null|        null|    null|         null|             null|\n|    1|         3|        3|2018-01-03 00:00:00|   MSY|     New Orleans, LA| JFK|   New York, NY|   1135|   -10.0|   1515|   -32.0|      0.0|            null|     0.0|            160.0|  140.0|  1182.0|        null|        null|    null|         null|             null|\n|    1|         1|        1|2018-01-01 00:00:00|   PPG|       Pago Pago, TT| HNL|   Honolulu, HI|   2319|   -11.0|    548|   -12.0|      0.0|            null|     0.0|            329.0|  310.0|  2599.0|        null|        null|    null|         null|             null|\n+-----+----------+---------+-------------------+------+--------------------+----+---------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "distinctFlights = flightsDF.distinct()  # distinct is a transformation returning a new DataFrame without duplicated rows\ndistinctFlightsCount = distinctFlights.count()\n\nprint(\"There are {0} distinctFlightsCount distinct rows\".format(distinctFlightsCount))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There are 2483783 distinctFlightsCount distinct rows\n"}], "metadata": {}}, {"source": "No hay dos filas exactamente iguales, pero si seleccionamos `Origin` y `Dest` entonces obtenemos un DataFrame con los posibles aeropuertos de origen y destino, es decir, aquellos trayectos que existen en un vuelo (pueden aparecer varias veces porque seguramente existir\u00e1n varios vuelos en enero de 2008 entre un origen y un destino)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "countOriginDests = flightsDF.select(\"Origin\", \"Dest\")\\\n                            .distinct().count()", "outputs": [], "metadata": {}}, {"source": "<div class=\"alert alert-block alert-success\">\n    <b>TU TURNO</b>: \u00bf<b>cu\u00e1ntas</b> combinaciones de Origin y Dest existen?\n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "countOriginDests", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "5795"}, "metadata": {}}], "metadata": {}}, {"source": "**Respuesta en la siguiente celda:**", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "print(\"Hay {0} combinaciones de un aeropuerto de origen y uno de destino\".format(countOriginDests))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Hay 5795 combinaciones de un aeropuerto de origen y uno de destino\n"}], "metadata": {"scrolled": true}}, {"source": "Aunque tengamos 2.5 millones de filas, solo hay 5795 combinaciones distintas de un aeropuerto de origen y otro de destino.\n\n<div class=\"alert alert-block alert-success\">\n    <b>TU TURNO</b>: \u00bf<b>cu\u00e1ntos</b> aeropuertos de origen existen?\n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "distinctOrigins = flightsDF.select(\"Origin\").distinct().count()\n\nprint(\"Hay {0} aeropuertos desde los que puede partir un vuelo\".format(distinctOrigins))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Hay 356 aeropuertos desde los que puede partir un vuelo\n"}], "metadata": {}}, {"source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: vamos a hacer esto para cada columna, en un bucle, para hacernos a la idea de cu\u00e1ntos valores hay. Esto solo tiene sentido en realidad para columnas categ\u00f3ricas y no para num\u00e9ricas donde casi todos los valores ser\u00e1n distintos.\n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "flightsDF.columns", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "['Month',\n 'DayofMonth',\n 'DayOfWeek',\n 'FlightDate',\n 'Origin',\n 'OriginCity',\n 'Dest',\n 'DestCity',\n 'DepTime',\n 'DepDelay',\n 'ArrTime',\n 'ArrDelay',\n 'Cancelled',\n 'CancellationCode',\n 'Diverted',\n 'ActualElapsedTime',\n 'AirTime',\n 'Distance',\n 'CarrierDelay',\n 'WeatherDelay',\n 'NASDelay',\n 'SecurityDelay',\n 'LateAircraftDelay']"}, "metadata": {}}], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "for columnName in flightsDF.columns:\n    distinctValues = flightsDF.select(columnName).distinct().count()\n    \n    # No olvid\u00e9is indentar este comando para indicar que est\u00e1 dentro del cuerpo del bucle\n    print(\"Existen {0} valores distintos en la columna {1}\".format(distinctValues, columnName))\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Existen 4 valores distintos en la columna Month\nExisten 31 valores distintos en la columna DayofMonth\nExisten 7 valores distintos en la columna DayOfWeek\nExisten 120 valores distintos en la columna FlightDate\nExisten 356 valores distintos en la columna Origin\nExisten 350 valores distintos en la columna OriginCity\nExisten 356 valores distintos en la columna Dest\nExisten 350 valores distintos en la columna DestCity\nExisten 1441 valores distintos en la columna DepTime\nExisten 1379 valores distintos en la columna DepDelay\nExisten 1441 valores distintos en la columna ArrTime\nExisten 1363 valores distintos en la columna ArrDelay\nExisten 2 valores distintos en la columna Cancelled\nExisten 6 valores distintos en la columna CancellationCode\nExisten 2 valores distintos en la columna Diverted\nExisten 706 valores distintos en la columna ActualElapsedTime\nExisten 670 valores distintos en la columna AirTime\nExisten 1466 valores distintos en la columna Distance\nExisten 1095 valores distintos en la columna CarrierDelay\nExisten 749 valores distintos en la columna WeatherDelay\nExisten 610 valores distintos en la columna NASDelay\nExisten 129 valores distintos en la columna SecurityDelay\nExisten 740 valores distintos en la columna LateAircraftDelay\n"}], "metadata": {}}, {"source": "### Crear una nueva columna o reemplazar una existente por el resultado de operar con columnas existentes", "cell_type": "markdown", "metadata": {}}, {"source": "Tenemos las distancias en millas. Vamos a convertirlas a km multiplicando las millas por 1.61. De nuevo, cada nodo del cluster har\u00e1 esta operaci\u00f3n localmente con los datos que se encuentran en ese nodo. El DataFrame resultante estar\u00e1 repartido en los mismos nodos. **No hay movimiento de datos ya que no se necesita ninguna informaci\u00f3n que no est\u00e9 presente en ese nodo** para efectuar la operaci\u00f3n.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "# withColumn is a transformation returing a new DataFrame with one extra column appended on the right\nflightsWithKm = flightsDF.withColumn(\"DistanceKm\", F.col(\"Distance\") * 1.61)\n\nflightsWithKm.printSchema()\n\nflightsWithKm.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n |-- DistanceKm: double (nullable = true)\n\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n|Month|DayofMonth|DayOfWeek|         FlightDate|Origin|  OriginCity|Dest|    DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|        DistanceKm|\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n|    1|        14|        7|2018-01-14 00:00:00|   SYR|Syracuse, NY| DTW| Detroit, MI|   null|    null|   null|    null|      1.0|               B|     0.0|             null|   null|   374.0|        null|        null|    null|         null|             null|            602.14|\n|    1|         3|        3|2018-01-03 00:00:00|   SYR|Syracuse, NY| LGA|New York, NY|   1348|   -10.0|   1506|      NA|      0.0|            null|     0.0|             78.0|   42.0|   198.0|        null|        null|    null|         null|             null|318.78000000000003|\n|    1|         6|        6|2018-01-06 00:00:00|   SYR|Syracuse, NY| LGA|New York, NY|   1410|    12.0|   1543|    24.0|      0.0|            null|     0.0|             93.0|   45.0|   198.0|        12.0|         0.0|    12.0|          0.0|              0.0|318.78000000000003|\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\nonly showing top 3 rows\n\n"}], "metadata": {}}, {"source": "El d\u00eda de la semana es una variable entera. Utilizar un entero o una variable categ\u00f3rica es una decisi\u00f3n que depende de qu\u00e9 tipo de modelo vayamos a ajustar. \u00bfTiene sentido considerar d\u00edas de la semana \"m\u00e1s grandes\" o \"m\u00e1s peque\u00f1os\", es decir, algo que se incrementa conforme \"se incrementa\" el d\u00eda de la semana de 1 a 7? Aqu\u00ed vamos a **reemplazar** la columna `DayOfWeek` que ya exist\u00eda, por una versi\u00f3n categ\u00f3rica como strings. Utilizamos `withColumn` pas\u00e1ndole como primer argumento el nombre de una columna que ya existe, lo cual indica que queremos reemplazarla, en el mismo lugar que ocupaba.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "flightsCategoricalDay = flightsDF.withColumn(\"DayOfWeek\", F.when(F.col(\"DayOfWeek\") == 1, \"Monday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 2, \"Tuesday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 3, \"Wednesday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 4, \"Thursday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 5, \"Friday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 6, \"Saturday\")\\\n                                                           .otherwise(\"Sunday\"))\n\nflightsCategoricalDay.printSchema() # the column is still in the same position but has now string type\n\nflightsCategoricalDay.select(\"DayOfWeek\", \"DepTime\", \"ArrTime\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: string (nullable = false)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n\n+---------+-------+-------+\n|DayOfWeek|DepTime|ArrTime|\n+---------+-------+-------+\n|   Sunday|   null|   null|\n|Wednesday|   1348|   1506|\n| Saturday|   1410|   1543|\n|   Sunday|   1347|   1455|\n|   Monday|   1350|   1509|\n|  Tuesday|   1351|   1504|\n|Wednesday|   1347|   1455|\n| Thursday|   1345|   1452|\n|   Friday|   1640|   1748|\n| Saturday|   1338|   1514|\n|   Monday|   1353|   1456|\n|  Tuesday|   1357|   1511|\n|Wednesday|   1526|   1622|\n| Thursday|   1354|   1509|\n|   Friday|   1344|   1449|\n| Saturday|   1433|   1533|\n|   Sunday|   1353|   1508|\n|   Monday|   1354|   1504|\n|  Tuesday|   1501|   1616|\n|Wednesday|   1355|   1515|\n+---------+-------+-------+\nonly showing top 20 rows\n\n"}], "metadata": {"scrolled": false}}, {"source": "<div class=\"alert alert-block alert-info\">\n    <b>CONSEJO:</b> El proceso de crear nuevas variables o <i>features</i> a partir de otras existentes, incluso incorporar variables de fuentes de datos externas (datos p\u00fablicos) y de limpiar o reemplazar variables tras normalizarla se denomina gen\u00e9ricamente <i>feature engineering</i> (ingenier\u00eda de variables). A veces tiene mucho que ver con conocimientos espec\u00edficos del dominio, aunque tambi\u00e9n con trucos estad\u00edsticos para normalizar siguiend m\u00e9todos bien conocidos.\n</div>", "cell_type": "markdown", "metadata": {}}, {"source": "La funci\u00f3n `when` es muy com\u00fan para re-categorizar una variable o para crear nuevas variables categ\u00f3ricas a partir de condiciones complejas acerca de lo que les ocurre a los valores de otras columnas en esa misma fila.", "cell_type": "markdown", "metadata": {}}, {"source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: crea una variable categ\u00f3rica de tipo string con dos categor\u00edas indicando si el d\u00eda de la semana es laborable o es fin de semana. Los valores deben ser \"laborable\" o \"finde\". Utiliza `withColumn` y `when`. Ser\u00e1 laborable cuando DayOfWeek est\u00e9 entre 1 y 5, y fin de semana cuando sea 6 o 7 (en resumen: \"en otro caso\" es fin de semana). \n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "flightsWithWorkingDF = flightsDF.withColumn(\"WorkDay\", F.when(F.col(\"DayOfWeek\") < 6 , \"laborable\")\\\n                                                        .otherwise(\"finde\"))\n                                            \nflightsWithWorkingDF.select(\"DayOfWeek\", \"WorkDay\").distinct().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+---------+\n|DayOfWeek|  WorkDay|\n+---------+---------+\n|        2|laborable|\n|        4|laborable|\n|        6|    finde|\n|        3|laborable|\n|        5|laborable|\n|        7|    finde|\n|        1|laborable|\n+---------+---------+\n\n"}], "metadata": {}}, {"source": "### Crear y seleccionar columnas al vuelo", "cell_type": "markdown", "metadata": {}}, {"source": "`withColumn` no es la \u00fanica manera de crear columnas. Podemos usar `select` no solo para seleccionar columnas existentes sino tambi\u00e9n para crear nuevas columnas al vuelo y a la vez seleccionarlas. Vamos a seleccionar Origin y Dest, que ya existen, a la vez que seleccionamos una tercera columna que estamos creando al vuelo, con la transformaci\u00f3n de la distancia a km. Le ponemos como nombre \"DistanceKm\" en ese momento que la estamos creando, mediante la funci\u00f3n `alias`. Si no usamos `alias`, Spark le pondr\u00e1 a la nueva columna un nombre por defecto que en este caso ser\u00eda \"1.6 * Distance\". Se recomienda usar alias para dar nombre a las nuevas columnas.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "flightsDF.selectExpr(\"Origin\", \"Dest\", \"1.6*Distance as DistanceKm\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------+----+----------+\n|Origin|Dest|DistanceKm|\n+------+----+----------+\n|   SYR| DTW|     598.4|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n+------+----+----------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "flightsAirportsAndKm = flightsDF.select(F.col(\"Origin\"),\\\n                                        F.col(\"Dest\"),\\\n                                        (1.6 * F.col(\"Distance\")).alias(\"DistanceKm\"))\n\nflightsAirportsAndKm.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------+----+----------+\n|Origin|Dest|DistanceKm|\n+------+----+----------+\n|   SYR| DTW|     598.4|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n+------+----+----------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: crear un DataFrame con tres columnas seleccionando \"Origin\", \"OriginCity\" y una nueva columna de tipo string creada concatenando Origin y OriginCity con un gui\u00f3n \"-\". Utilizar la funci\u00f3n <i>concat_ws</i>(concatenar con separador) con sintaxis F.concat_ws(\"-\", columna1, columna2) del paquete pyspark.sql.functions.\n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "flightsOriginCity = flightsDF.select(\"Origin\", \"OriginCity\", F.concat_ws(\"-\", \"Origin\", \"OriginCity\").alias(\"O-T\"))\n# Tambi\u00e9n sirve: \n# flightsNumTail = flightsDF.select(F.col(\"FlightNum\"), \n#                                   F.col(\"TailNum\"), \n#                                  F.concat_ws(\"-\", F.col(\"FlightNum\"), F.col(\"TailNum\")).alias(\"concat\"))", "outputs": [], "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "flightsOriginCity.printSchema()\nflightsOriginCity.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- O-T: string (nullable = false)\n\n+------+------------+----------------+\n|Origin|  OriginCity|             O-T|\n+------+------------+----------------+\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n+------+------------+----------------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"source": "### Convertir el tipo de dato de una columna", "cell_type": "markdown", "metadata": {}}, {"source": "Con frecuencia, Spark no infiere correctamente el tipo de cada columna. \n* A veces ocurre que una columna num\u00e9rica no se reconozca adecuadamente porque los datos tienen \"NA\" (un string) que quer\u00eda significar data faltante en el dataset original. \"NA\" no es un string especial para Spark, por lo que simplemente reconoce que la columna tiene tanto enteros como strings, y el tipo m\u00e1s general que infiere es string para esa columna. \n* Pero no es el caso aqu\u00ed porque no se da esto. No obstante vamos a convertir el d\u00eda de la semana a string.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "naCount = flightsDF.where(\"ArrDelay = 'NA'\").count()\n\n# Esto es sintaxis SQL, pero tambi\u00e9n podr\u00edamos haberla llamado como .where(F.col(\"ArrDelay\") == \"NA\"). Ambas son equivalentes.\n\nprint(\"Hay \", naCount, \"filas con NA en ArrDelay\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "('Hay ', 250008, 'filas con NA en ArrDelay')\n"}], "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "flightsDF.where(F.col(\"ArrDelay\") != \"NA\").count()", "outputs": [{"execution_count": 26, "output_type": "execute_result", "data": {"text/plain": "2196438"}, "metadata": {}}], "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "from pyspark.sql.types import DoubleType\n\nflightsDF = flightsDF.where((F.col(\"ArrDelay\") != \"NA\"))\\\n                     .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(DoubleType()))", "outputs": [], "metadata": {}}, {"source": "### Ordenaci\u00f3n respecto a una columna", "cell_type": "markdown", "metadata": {}}, {"source": "Es una transformaci\u00f3n que nos devuelve otro DataFrame ordenado seg\u00fan la(s) columna(s) indicada(s), sea de forma ascendente o descendente. El DataFrame original no se modifica (al igual que ocurre en cualquier transformaci\u00f3n). Se puede ordenar en base a una columna num\u00e9rica (lo m\u00e1s frecuente) o tambi\u00e9n categ\u00f3rica (los strings se ordenan alfab\u00e9ticamente). Para ordenar s\u00ed es necesario enviar informaci\u00f3n de unos nodos a otros puesto que no sabemos si existen o no datos mayores o menos que el valor que tenemos en el nodo (o m\u00e1s precisamente, en el worker).", "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "# Ordenamos los vuelos seg\u00fan ArrDelay\nsortedDF = flightsDF.orderBy(\"ArrDelay\")  # equivalente: flightsDF.orderBy(F.col(\"ArrDelay\"))\n\nsortedDescDF = flightsDF.orderBy(\"ArrDelay\", ascending = False)\nsortedDescDF.select(\"ArrDelay\", \"Origin\", \"Dest\").show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+------+----+\n|ArrDelay|Origin|Dest|\n+--------+------+----+\n|  2475.0|   HNL| PPG|\n|  2454.0|   PPG| HNL|\n|  2023.0|   EGE| JFK|\n|  1778.0|   ORF| DFW|\n|  1757.0|   SMF| DFW|\n|  1717.0|   HNL| JFK|\n|  1704.0|   EGE| DFW|\n|  1685.0|   ORF| JFK|\n|  1650.0|   ABQ| DFW|\n|  1648.0|   SLC| DFW|\n+--------+------+----+\nonly showing top 10 rows\n\n"}], "metadata": {}}, {"source": "### Funciones de agregaci\u00f3n sobre el DataFrame completo", "cell_type": "markdown", "metadata": {}}, {"source": "Spark tiene implementaciones distribuidas y paralelas de funciones de agregaci\u00f3n frecuentes como la media, min, max y desviaci\u00f3n t\u00edpica entre otras. Todas estas funciones reciben como argumento el objeto columna sobre el que la funci\u00f3n debe aplicarse. Lo m\u00e1s habitual es aplicarlas para agregar por grupos, pero esto lo veremos en el segundo notebook.", "cell_type": "markdown", "metadata": {}}, {"source": "Vamos a seleccionar vuelos con `ArrDelay` mayor de 15 minutos, y con ellos vamos a crear columnas con el min, max, media y desviaci\u00f3n t\u00edpica de la columna `ArrDelay`. Crearemos y seleccionaremos dichas columnas al vuelo, como vimos en la secci\u00f3n anterior.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "flightsDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: double (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n\n"}], "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "flightsDF.count()", "outputs": [{"execution_count": 30, "output_type": "execute_result", "data": {"text/plain": "2196438"}, "metadata": {}}], "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "# First we select those flights with at least 16 minutes of delay and then compute the aggregations\nflightsDF.where(F.col(\"ArrDelay\") > 15)\\\n         .select(F.mean(\"ArrDelay\").alias(\"MeanArrDelay\"),\\\n                 F.min(\"ArrDelay\").alias(\"MinArrDelay\"),\\\n                 F.max(\"ArrDelay\").alias(\"MaxArrDelay\"),\n                 F.stddev(\"ArrDelay\").alias(\"StddevArrDelay\")).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------+-----------+-----------+-----------------+\n|    MeanArrDelay|MinArrDelay|MaxArrDelay|   StddevArrDelay|\n+----------------+-----------+-----------+-----------------+\n|65.0029554145525|       16.0|     2475.0|82.39508021279889|\n+----------------+-----------+-----------+-----------------+\n\n"}], "metadata": {}}, {"source": "Existe una funci\u00f3n de Spark que hace casi todo esto por nosotros, llamada `summary`. Lo hace para cada columna num\u00e9rica que encuentre en el dataset. Es tambi\u00e9n una transformaci\u00f3n que devuelve un nuevo DataFrame con las m\u00e9tricas de resumen, sin modificar el DataFrame original.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "summariesDF = flightsDF.summary()\nsummariesDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+------------------+------------------+-------+------------+-------+------------+------------------+------------------+------------------+------------------+---------+----------------+--------+------------------+------------------+-----------------+------------------+-----------------+------------------+-------------------+------------------+\n|summary|             Month|        DayofMonth|         DayOfWeek| Origin|  OriginCity|   Dest|    DestCity|           DepTime|          DepDelay|           ArrTime|          ArrDelay|Cancelled|CancellationCode|Diverted| ActualElapsedTime|           AirTime|         Distance|      CarrierDelay|     WeatherDelay|          NASDelay|      SecurityDelay| LateAircraftDelay|\n+-------+------------------+------------------+------------------+-------+------------+-------+------------+------------------+------------------+------------------+------------------+---------+----------------+--------+------------------+------------------+-----------------+------------------+-----------------+------------------+-------------------+------------------+\n|  count|           2196438|           2196438|           2196438|2196438|     2196438|2196438|     2196438|           2196438|           2194537|           2196437|           2196438|  2196438|               0| 2196438|           2196436|           2193832|          2196438|            392026|           392026|            392026|             392026|            392026|\n|   mean| 2.542641768171922|15.653609617025383|3.9118777766547472|   null|        null|   null|        null|    1334.619922347| 8.757860541881955|1474.1815007669238|3.2180216332079485|      0.0|            null|     0.0|133.32671473241197|108.49199255002206|767.0018985284356|20.025480452827107|3.380240086116737|14.401940177437211|0.08158387453893365|25.262087208501477|\n| stddev|1.1249716899298523| 8.704384106280886| 2.004575196819417|   null|        null|   null|        null|498.65801497243547|43.293119748201676| 527.5834382161728|45.553015921702126|      0.0|            null|     0.0| 72.04367493737419| 70.26234053020681|584.2246262771182| 60.08817003361008|30.75284516680867| 32.27787390403157| 2.4940266386473477|49.824896393449514|\n|    min|                 1|                 1|                 1|    ABE|Aberdeen, SD|    ABE|Aberdeen, SD|                 1|           -1280.0|                 1|           -1290.0|      0.0|            null|     0.0|           -1228.0|           -1244.0|             16.0|               0.0|              0.0|               0.0|                0.0|               0.0|\n|    25%|                 2|                 8|                 2|   null|        null|   null|        null|               920|              -6.0|              1055|             -15.0|      0.0|            null|     0.0|              82.0|              58.0|            337.0|               0.0|              0.0|               0.0|                0.0|               0.0|\n|    50%|                 3|                16|                 4|   null|        null|   null|        null|              1329|              -3.0|              1510|              -7.0|      0.0|            null|     0.0|             115.0|              90.0|            604.0|               1.0|              0.0|               2.0|                0.0|               2.0|\n|    75%|                 4|                23|                 6|   null|        null|   null|        null|              1741|               6.0|              1914|               7.0|      0.0|            null|     0.0|             164.0|             138.0|           1009.0|              17.0|              0.0|              19.0|                0.0|              30.0|\n|    max|                 4|                31|                 7|    YUM|    Yuma, AZ|    YUM|    Yuma, AZ|              2400|            2482.0|              2400|            2475.0|      0.0|            null|     0.0|             757.0|             696.0|           4983.0|            2007.0|           2475.0|            1515.0|              593.0|            2454.0|\n+-------+------------------+------------------+------------------+-------+------------+-------+------------+------------------+------------------+------------------+------------------+---------+----------------+--------+------------------+------------------+-----------------+------------------+-----------------+------------------+-------------------+------------------+\n\n"}], "metadata": {}}, {"source": "### Conversi\u00f3n a Pandas", "cell_type": "markdown", "metadata": {}}, {"source": "En general es complicado leer los resultados que muestra Spark. En ocasiones interesa convertirlos a un dataframe de Pandas, aunque esto implica traer todas las filas al driver, lo cual **debe hacerse con mucho cuidado y solo en casos en los que estemos seguros de que el DataFrame de Spark es peque\u00f1o y no desbordar\u00e1 la memoria del driver**.", "cell_type": "markdown", "metadata": {}}, {"source": "<div class=\"alert alert-block alert-info\">\n<b>CONSEJO</b>: el concepto de dataframe como tabla con filas y columnas existe en muchos lenguajes de programaci\u00f3n (pandas de Python, dataframes de R, DataFrames de Spark). Sin embargo, Spark maneja DataFrames que est\u00e1n f\u00edsicamente distribuidos en las memorias RAM de las m\u00e1quinas del cluster, lo cual no tiene nada que ver con lo que hace la biblioteca Pandas o R que se ejecutan en una sola m\u00e1quina. Convertir un DataFrame de Spark en un dataframe de Pandas implica llevar todas las filas al driver, lo cual podr\u00eda resultar en una excepci\u00f3n Out-of-Memory si el contenido del DataFrame es m\u00e1s grande que la memoria RAM de la m\u00e1quina en la que se est\u00e1 ejecutando el programa driver. En este caso y en la mayor\u00eda de casos, se suele utilizar para mostrar res\u00famenes o agregados que ocupan poco, con lo que no existe riesgo.\n\nEsta operaci\u00f3n tambi\u00e9n es muy frecuente cuando queremos representar gr\u00e1ficamente el contenido de un DataFrame de Spark. No existen funciones gr\u00e1ficas en Spark, por lo que tenemos que convertirlo en un dataframe de Pandas y utilizar las funciones gr\u00e1ficas de Python habituales (matplotlib o incluso la propia biblioteca Pandas) para mostrar lo que necesitemos.\n</div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "flightsPd = flightsDF.summary().toPandas()", "outputs": [], "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "flightsPd", "outputs": [{"execution_count": 34, "output_type": "execute_result", "data": {"text/plain": "  summary               Month          DayofMonth           DayOfWeek  \\\n0   count             2196438             2196438             2196438   \n1    mean   2.542641768171922  15.653609617025383  3.9118777766547472   \n2  stddev  1.1249716899298523   8.704384106280886   2.004575196819417   \n3     min                   1                   1                   1   \n4     25%                   2                   8                   2   \n5     50%                   3                  16                   4   \n6     75%                   4                  23                   6   \n7     max                   4                  31                   7   \n\n    Origin    OriginCity     Dest      DestCity             DepTime  \\\n0  2196438       2196438  2196438       2196438             2196438   \n1     None          None     None          None      1334.619922347   \n2     None          None     None          None  498.65801497243547   \n3      ABE  Aberdeen, SD      ABE  Aberdeen, SD                   1   \n4     None          None     None          None                 920   \n5     None          None     None          None                1329   \n6     None          None     None          None                1741   \n7      YUM      Yuma, AZ      YUM      Yuma, AZ                2400   \n\n             DepDelay         ...         CancellationCode Diverted  \\\n0             2194537         ...                        0  2196438   \n1   8.757860541881955         ...                     None      0.0   \n2  43.293119748201676         ...                     None      0.0   \n3             -1280.0         ...                     None      0.0   \n4                -6.0         ...                     None      0.0   \n5                -3.0         ...                     None      0.0   \n6                 6.0         ...                     None      0.0   \n7              2482.0         ...                     None      0.0   \n\n    ActualElapsedTime             AirTime           Distance  \\\n0             2196436             2193832            2196438   \n1  133.32671473241197  108.49199255002206  767.0018985284356   \n2   72.04367493737419   70.26234053020681  584.2246262771182   \n3             -1228.0             -1244.0               16.0   \n4                82.0                58.0              337.0   \n5               115.0                90.0              604.0   \n6               164.0               138.0             1009.0   \n7               757.0               696.0             4983.0   \n\n         CarrierDelay       WeatherDelay            NASDelay  \\\n0              392026             392026              392026   \n1  20.025480452827107  3.380240086116737  14.401940177437211   \n2   60.08817003361008  30.75284516680867   32.27787390403157   \n3                 0.0                0.0                 0.0   \n4                 0.0                0.0                 0.0   \n5                 1.0                0.0                 2.0   \n6                17.0                0.0                19.0   \n7              2007.0             2475.0              1515.0   \n\n         SecurityDelay   LateAircraftDelay  \n0               392026              392026  \n1  0.08158387453893365  25.262087208501477  \n2   2.4940266386473477  49.824896393449514  \n3                  0.0                 0.0  \n4                  0.0                 0.0  \n5                  0.0                 2.0  \n6                  0.0                30.0  \n7                593.0              2454.0  \n\n[8 rows x 23 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>Month</th>\n      <th>DayofMonth</th>\n      <th>DayOfWeek</th>\n      <th>Origin</th>\n      <th>OriginCity</th>\n      <th>Dest</th>\n      <th>DestCity</th>\n      <th>DepTime</th>\n      <th>DepDelay</th>\n      <th>...</th>\n      <th>CancellationCode</th>\n      <th>Diverted</th>\n      <th>ActualElapsedTime</th>\n      <th>AirTime</th>\n      <th>Distance</th>\n      <th>CarrierDelay</th>\n      <th>WeatherDelay</th>\n      <th>NASDelay</th>\n      <th>SecurityDelay</th>\n      <th>LateAircraftDelay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2196438</td>\n      <td>2194537</td>\n      <td>...</td>\n      <td>0</td>\n      <td>2196438</td>\n      <td>2196436</td>\n      <td>2193832</td>\n      <td>2196438</td>\n      <td>392026</td>\n      <td>392026</td>\n      <td>392026</td>\n      <td>392026</td>\n      <td>392026</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>2.542641768171922</td>\n      <td>15.653609617025383</td>\n      <td>3.9118777766547472</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1334.619922347</td>\n      <td>8.757860541881955</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>133.32671473241197</td>\n      <td>108.49199255002206</td>\n      <td>767.0018985284356</td>\n      <td>20.025480452827107</td>\n      <td>3.380240086116737</td>\n      <td>14.401940177437211</td>\n      <td>0.08158387453893365</td>\n      <td>25.262087208501477</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stddev</td>\n      <td>1.1249716899298523</td>\n      <td>8.704384106280886</td>\n      <td>2.004575196819417</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>498.65801497243547</td>\n      <td>43.293119748201676</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>72.04367493737419</td>\n      <td>70.26234053020681</td>\n      <td>584.2246262771182</td>\n      <td>60.08817003361008</td>\n      <td>30.75284516680867</td>\n      <td>32.27787390403157</td>\n      <td>2.4940266386473477</td>\n      <td>49.824896393449514</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>ABE</td>\n      <td>Aberdeen, SD</td>\n      <td>ABE</td>\n      <td>Aberdeen, SD</td>\n      <td>1</td>\n      <td>-1280.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>-1228.0</td>\n      <td>-1244.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25%</td>\n      <td>2</td>\n      <td>8</td>\n      <td>2</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>920</td>\n      <td>-6.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>82.0</td>\n      <td>58.0</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50%</td>\n      <td>3</td>\n      <td>16</td>\n      <td>4</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1329</td>\n      <td>-3.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>115.0</td>\n      <td>90.0</td>\n      <td>604.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>75%</td>\n      <td>4</td>\n      <td>23</td>\n      <td>6</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1741</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>164.0</td>\n      <td>138.0</td>\n      <td>1009.0</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>19.0</td>\n      <td>0.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>max</td>\n      <td>4</td>\n      <td>31</td>\n      <td>7</td>\n      <td>YUM</td>\n      <td>Yuma, AZ</td>\n      <td>YUM</td>\n      <td>Yuma, AZ</td>\n      <td>2400</td>\n      <td>2482.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>757.0</td>\n      <td>696.0</td>\n      <td>4983.0</td>\n      <td>2007.0</td>\n      <td>2475.0</td>\n      <td>1515.0</td>\n      <td>593.0</td>\n      <td>2454.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows \u00d7 23 columns</p>\n</div>"}, "metadata": {}}], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}