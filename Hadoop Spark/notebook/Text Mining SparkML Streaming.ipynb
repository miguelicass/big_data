{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Aprendizaje supervisado: un problema de clasificación\n",
    "\n",
    "Los algoritmos de aprendizaje supervisado utilizan datos etiquetados en los que tanto la entrada como el resultado objetivo (*etiqueta*), se proporcionan al algoritmo. El aprendizaje supervisado también se denomina modelado predictivo o análisis predictivo, porque crea un modelo que es capaz de realizar predicciones.\n",
    "\n",
    "Un algoritmo de clasificación toma un conjunto de datos con etiquetas conocidas y características predeterminadas, y aprende cómo etiquetar nuevos registros en función de esa información. Las características definen a cada individuo (cada registro, fila de nuestros datos, también llamado *ejemplo*). La etiqueta es la salida que corresponde con esas características. \n",
    "\n",
    "Veamos un ejemplo de clasificación de texto. \n",
    "* ¿Qué estamos tratando de predecir?\n",
    "  * Si una revisión de producto es positiva o negativa.\n",
    "  * Retrasada es la etiqueta: 1 para positivo 0 para negativo\n",
    "* ¿Cuáles son las propiedades que puede utilizar para hacer predicciones?\n",
    "  * Las palabras del texto de revisión se utilizan como características para descubrir similitudes y categorizar el sentimiento del texto del cliente como positivo o negativo.\n",
    "\n",
    "### Regresión logística\n",
    "\n",
    "La regresión logística es un método popular para predecir una respuesta binaria. Es un caso especial de modelos lineales generalizados que predice la probabilidad de que la clase asociada a un ejemplo sea una de las clases, o bien la otra (suele usarse casi siempre en problemas donde los registros pertenecen a una de entre dos clases posibles). La regresión logística mide la relación entre la \"etiqueta\" ***Y*** y las \"características\" ***X*** a través la estimación de probabilidades mediante una función logística. El modelo predice una probabilidad que se utiliza para predecir la clase a la que pertenece ese ejemplo.\n",
    "\n",
    "### Dataset de opiniones de Amazon\n",
    "Tenemos un conjunto de datos formado por textos breves escritos por clientes de Amazon al recibir su compra, en los cuales cada cliente expresa su opinión sobre el producto adquirido. Cada registro (cada fila del dataset) representa la opinión frente a algún producto. El texto tiene, por un lado, una columna en la que el cliente da un titular o resumen a su revisión, y por otro, una columna con un texto más largo donde expresa el detalle.\n",
    "\n",
    "El dataset se puede descargar desde \n",
    "http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\n",
    "\n",
    "Se encuentra en formato JSON-line en el que cada línea es un JSON completo, como el del siguiente ejemplo:\n",
    "\n",
    "`{\"reviewerID\": \"A1PUWI9RTQV19S\", \"asin\": \"B003Y5C132\", \"reviewerName\": \"kris\", \"helpful\": [0, 1], \"reviewText\": \"A little small in hind sight, but I did order a .30 cal box. Good condition, and keeps my ammo organized.\", \"overall\": 5.0, \"summary\": \"Nice ammo can\", \"unixReviewTime\": 1384905600, \"reviewTime\": \"11 20, 2013\"}`\n",
    "\n",
    "que, como vemos, sigue el siguiente esquema:\n",
    "\n",
    "* **reviewerID** - identificador del cliente, p.ej. A2SUAM1J3GNN3B\n",
    "* **asin** - identificador del producto, p.ej. 0000013714\n",
    "* **reviewerName** - nombre del cliente\n",
    "* **helpful** - valoración del grado de utilidad de esta opinión, expresado como un número real entre 0 y 1, p.ej. 2/3\n",
    "* **reviewText** - texto de la opinión\n",
    "* **overall** - valoración que da el cliente al producto, entre 1 y 5\n",
    "* **summary** - resumen de la revisión\n",
    "* **unixReviewTime** - instante en el que se creó esta opinión (expresado como unix time)\n",
    "* **reviewTime** - instante en el que se creó esta opinión (formato en crudo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------+--------------------+\n",
      "|      asin|overall|          reviewText|             summary|unixReviewTime|            reviewTS|\n",
      "+----------+-------+--------------------+--------------------+--------------+--------------------+\n",
      "|1881509818|    5.0|This came in on t...|      Woks very good|    1390694400|Woks very good Th...|\n",
      "|1881509818|    5.0|I had a factory G...|Works as well as ...|    1328140800|Works as well as ...|\n",
      "|1881509818|    4.0|If you don't have...|It's a punch, tha...|    1330387200|It's a punch, tha...|\n",
      "|1881509818|    4.0|This works no bet...|It's a punch with...|    1328400000|It's a punch with...|\n",
      "|1881509818|    4.0|I purchased this ...|Ok,tool does what...|    1366675200|Ok,tool does what...|\n",
      "+----------+-------+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawDF = spark.read\\\n",
    "           .option(\"inferSchema\", \"true\")\\\n",
    "           .json(\"gs://unir-ipmd/datos/Sports_and_Outdoors_5.json\")\n",
    "\n",
    "# add column combining summary and review text, drop some others \n",
    "df = rawDF.withColumn(\"reviewTS\",\n",
    "                      F.concat(F.col(\"summary\"), F.lit(\" \"), F.col(\"reviewText\")))\\\n",
    "          .drop(\"helpful\", \"reviewerID\", \"reviewerName\", \"reviewTime\")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- reviewTS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a quitar las opiniones neutras (con valoración 3 sobre 5) para evitar posibles confusiones. Cualquier opinión con valoración 1 ó 2 será considerada negativa, mientras que una opinión con valoración 4 ó 5 será considerada positiva. Estas dos clases (negativa y positiva) serán los posibles valores de nuestra columna objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------+--------------------+\n",
      "|      asin|overall|          reviewText|             summary|unixReviewTime|            reviewTS|\n",
      "+----------+-------+--------------------+--------------------+--------------+--------------------+\n",
      "|1881509818|    5.0|This came in on t...|      Woks very good|    1390694400|Woks very good Th...|\n",
      "|1881509818|    5.0|I had a factory G...|Works as well as ...|    1328140800|Works as well as ...|\n",
      "|1881509818|    4.0|If you don't have...|It's a punch, tha...|    1330387200|It's a punch, tha...|\n",
      "|1881509818|    4.0|This works no bet...|It's a punch with...|    1328400000|It's a punch with...|\n",
      "|1881509818|    4.0|I purchased this ...|Ok,tool does what...|    1366675200|Ok,tool does what...|\n",
      "|1881509818|    5.0|Needed this tool ...|Glock punch tool ...|    1351814400|Glock punch tool ...|\n",
      "|1881509818|    5.0|If u don't have i...|          Great tool|    1402358400|Great tool If u d...|\n",
      "|2094869245|    4.0|This light will n...|             Bright!|    1377907200|Bright! This ligh...|\n",
      "|2094869245|    5.0|Light and laser t...|             Be seen|    1369612800|Be seen Light and...|\n",
      "|2094869245|    5.0|Does everything i...|Bicycle rear tail...|    1383350400|Bicycle rear tail...|\n",
      "|2094869245|    4.0|Very bright.  I w...|          Great lite|    1399420800|Great lite Very b...|\n",
      "|2094869245|    5.0|Mine arrived with...|For $11, it's a b...|    1389657600|For $11, it's a b...|\n",
      "|2094869245|    4.0|It works great it...|               Bulky|    1387497600|Bulky It works gr...|\n",
      "|2094869245|    5.0|I love this light...|            Love it!|    1379462400|Love it! I love t...|\n",
      "|2094869245|    5.0|Bit bulky. One bu...|       Bulky but....|    1389830400|Bulky but.... Bit...|\n",
      "|2094869245|    5.0|it is bright and ...|     rear bike light|    1386374400|rear bike light i...|\n",
      "|2094869245|    4.0|A mice bright lig...|Needed a little m...|    1383523200|Needed a little m...|\n",
      "|2094869245|    4.0|Had one ride on t...|Good light for th...|    1384214400|Good light for th...|\n",
      "|7245456259|    2.0|So it worked well...|resistance was go...|    1395964800|resistance was go...|\n",
      "|7245456259|    5.0|My girlfriend is ...|Girlfriend loves ...|    1378339200|Girlfriend loves ...|\n",
      "+----------+-------+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_neutral_df = df.filter(\"overall !=3\")\n",
    "no_neutral_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `describe()` nos da estadísticas de resumen acerca de una o varias columnas numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           overall|\n",
      "+-------+------------------+\n",
      "|  count|            272266|\n",
      "|   mean|  4.51664548639933|\n",
      "| stddev|0.9344777791100664|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_neutral_df.describe(\"overall\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|    1.0|  9045|\n",
      "|    4.0| 64809|\n",
      "|    2.0| 10204|\n",
      "|    5.0|188208|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_neutral_df.groupBy(\"overall\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión de la valoración numérica en una etiqueta binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear, a partir de la columna `overall` que contiene la valoración numérica, una nueva columna binaria llamada `label` que será la que utilice nuestros algoritmo predictivo. Para ello utilizaremos un `Binarizer` de Spark, fijando el umbral en 3.0 (que nunca se da en nuestros datos porque ya lo habíamos quitado). Todo valor por debajo de este umbral será considerado como 0.0 y todo valor por encima será convertido en 1.0. La columna original `overall` no se modifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|overall|label| count|\n",
      "+-------+-----+------+\n",
      "|    2.0|  0.0| 10204|\n",
      "|    5.0|  1.0|188208|\n",
      "|    1.0|  0.0|  9045|\n",
      "|    4.0|  1.0| 64809|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "import numpy as np\n",
    "\n",
    "binarizer = Binarizer(inputCol = \"overall\",\n",
    "                       outputCol = \"label\",\n",
    "                        threshold = 3.0)\n",
    "\n",
    "binary_target_df = binarizer.transform(no_neutral_df)\n",
    "binary_target_df.groupBy(\"overall\",\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muestreo estratificado\n",
    "Como suele ser habitual en los problemas de clasificación binaria, existen muchos más ejemplos pertenecientes a una clase (en este caso la clase positiva) que a otra. Para que el modelo también sea sensible a ejemplos de la clase negativa, es conveniente tratar de equilibrar la proporción de ejemplos de cada clase presentes en nuestro conjunto de datos. Hay varias estrategias para conseguir esto. Aquí optamos por la más simple (y a la vez, la menos recomendable en problemas reales) que es eliminar ejemplos de la clase mayoritaria.\n",
    "\n",
    "Utilizamos la función `sampleBy()` indicando la fracción de ejemplos de cada clase que queremos mantener. En este caso queremos mantener todos los ejemplos de la clase negativa (que son minoría), pero tan sólo queremos mantener el 10 % de los ejemplos de la clase mayoritaria. Si mostramos la cantidad de ejemplos en el DataFrame resultante de este muestreo, vemos que están más equilibrados aunque aún sigue ligeramente inclinado hacia la clase 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|19249|\n",
      "|  1.0|25134|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fractions = {1.0 : .1, 0.0 : 1.0}\n",
    "balanced_df = binary_target_df.stat.sampleBy(\"label\", fractions, 36)\n",
    "balanced_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder saber cómo de bien funcionará el modelo entrenado en datos nuevos nunca vistos, vamos a dividir el conjunto de datos en subconjuntos de entrenamiento y de test. El conjunto de test se utilizará una sola vez, al final, cuando ya tengamos decidido y entrenado el modelo de predicción. El objetivo del conjunto de test será calcular una métrica que estime la bondad del modelo cuando sea puesto en producción y empiece a predecir datos sobre los que realmente no se conoce su etiqueta.\n",
    "\n",
    "Usamos el 80 % de nuestros datos para entrenar, y el 20 % los dejamos fuera porque serán el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|15381|\n",
      "|  1.0|20132|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_seed = 5043\n",
    "training_data, test_data = balanced_df.randomSplit([0.8, 0.2], split_seed)\n",
    "\n",
    "training_data.cache()\n",
    "training_data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de variables y pipelines\n",
    "\n",
    "Para que las características sean utilizadas por un algoritmo de aprendizaje automático, deben transformarse y colocarse en vectores de características, que son vectores numéricos que representa el valor de cada característica. Los textos en sí mismos no son utilizables por los algoritmos hasta que no pasen a través de dicho proceso.\n",
    "\n",
    "Spark ML proporciona un conjunto uniforme de API de alto nivel creadas sobre DataFrames. Usaremos un ML Pipeline para pasar los datos a través de transformadores con el fin de extraer las características y un estimador para producir el modelo.\n",
    "\n",
    "* Transformador: Un transformador es un algoritmo que transforma un DataFrame en otro DataFrame. Usaremos un transformador para obtener un DataFrame con una columna de vector de características.\n",
    "\n",
    "* Estimador: un estimador es un algoritmo que se puede ajustar a un DataFrame para producir un transformador. Usaremos un estimador que consistirá en un algoritmo de Regresión logística para entrenar un modelo. El modelo entrenado obtenido será un transformador que es capaz de transformar datos sobre los que no se conoce su etiqueta, para calcular predicciones.\n",
    "\n",
    "* Pipeline: un pipeline encadena varios transformadores y estimadores para especificar un flujo de trabajo de aprendizaje automático. Usaremos un Pipeline de Spark ML para tener en una sola pieza toda la secuencia de transformaciones necesarias para preparar los datos hasta llegar al modelo. De esa manera, podemos entrenar la pieza (el pipeline) como un todo, y utilizarlo para que los datos nuevos también pasen a través de las mismas etapas que habíamos utilizado para preparar los datos de entrenamiento. Esto permite pre-procesar datos nuevos de la misma manera que se hizo con los datos de entrenamiento, siguiendo exactamente los mismos pasos.\n",
    "\n",
    "Por último utilizaremos un evaluador para medir la bondad del modelo entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos con las siguientes etapas de ingeniería de variables:\n",
    "\n",
    "* Primero utilizamos un `RegexTokenizer` para separar cada texto en palabras. Esto transforma cada texto en un vector de strings con las palabras. Para más detalles: http://spark.apache.org/docs/latest/ml-features.html#tokenizer\n",
    "* Después aplicaremos un `StopWordsRemover` para eliminar de cada vector de palabras aquellas sin significado, como artículos, preposiciones, etc. Para más detalles: http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol = \"reviewTS\",\n",
    "                               outputCol = \"reviewTokensUf\",\n",
    "                               pattern = \"\\\\s+|[,.()\\\"]\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol = \"reviewTokensUf\",\n",
    "                           outputCol = \"reviewTokens\"\n",
    "                          ).setStopWords(StopWordsRemover.loadDefaultStopWords(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos el siguiente método para convertir vectores de palabras de un texto en vectores numéricos, utilizables por un algoritmo predictivo.\n",
    "\n",
    "**De la documentación oficial de Spark**:\n",
    "\n",
    "TF-IDF es un método de vectorización de características ampliamente utilizado en la minería de texto para reflejar la importancia de un término para un documento en el corpus. Si denotamos a un término (palabra) como *t*, un documento como *d* y el corpus como *D*, entonces:\n",
    "\n",
    "* La Frecuencia de un término **TF(*t*, *d*)** es el número de veces que el término *t* aparece en el documento *d* \n",
    "* La frecuencia en el documento **DF(*t*, *D*)** es el número de documentos que contienen el término *t*.\n",
    "\n",
    "Si solo usamos la frecuencia de los términos para medir la importancia, es muy fácil sobreenfatizar erróneamente los términos que aparecen con mucha frecuencia pero que contienen poca información sobre el documento, p. Ej. la palabra *fútbol* en un corpus compuesto por biografías de futbolistas. Si un término aparece con mucha frecuencia en el corpus, significa que no contiene información especial sobre un documento en particular. \n",
    "\n",
    "* La *frecuencia inversa de los documentos* **IDF(*t*, *D*)** es una medida numérica de cuánta información proporciona un término:\n",
    "\n",
    "$$ IDF (t, D) = \\frac{log | D | +1} {DF (t, D) +1} $$\n",
    "\n",
    "donde | D | es el número total de documentos del corpus. Dado que se usa logaritmo, si un término aparece en todos los documentos, su valor IDF se convierte en 0. Tenga en cuenta que se aplica un término de suavizado para evitar dividir por cero para los términos fuera del corpus.\n",
    "\n",
    "La medida TF-IDF es simplemente el producto de TF e IDF:\n",
    "$$ TFIDF (t, d, D) = TF (t, d) ⋅ IDF (t, D) $$\n",
    "\n",
    "Hay varias variantes en la definición de TF y de IDF. En Spark ML, están separados para que sean flexibles y poder combinarlos de varias maneras.\n",
    "\n",
    "* `CountVectorizer()` cuenta las ocurrencias totales de cada palabra en todo el corpus de textos, y se queda con las N más relevantes, siendo N un parámetro especificado por el usuario (en nuestro caso, N = 20000). Tras esto, en cada texto contará el número de apariciones de cada una de esas N palabras seleccionadas. Por tanto, cada texto vendrá representado por un vector numérico de longitud 20000, y nuestro problema tendrá 20000 variables.\n",
    "\n",
    "* `HashingTF()` es similar, pero cada posición no se asocia a una sola palabra sino que puede estar compartida por más de una. El usuario especifica también la dimensión N de los vectores obtenidos (se recomienda que sea una potencia de 2 debido a cómo actúa esta técnica). A grandes rasgos, cada palabra se codifica mediante un código que a su vez va a parar a una posición determinada del vector numérico que va a representar a ese texto, por lo que puede haber colisiones en algunas ocasiones, y que una posición sea utilizada para acumular las apariciones de más de una palabra diferente.\n",
    "\n",
    "Se puede utilizar cualquiera de estas opciones, aunque no las dos simultáneamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF, HashingTF\n",
    "\n",
    "count_vectorizer = CountVectorizer().setInputCol(\"reviewTokens\")\\\n",
    "                                    .setOutputCol(\"cv\")\\\n",
    "                                    .setVocabSize(20000).setMinDF(4)\n",
    "\n",
    "idf = IDF().setInputCol(\"cv\").setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# El último elemento del pipeline será un estimador, concretamente de la clase LogisticRegression\n",
    "logisticRegression = LogisticRegression().setMaxIter(100)\\\n",
    "                                 .setRegParam(0.02)\\\n",
    "                                 .setElasticNetParam(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuramos el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, count_vectorizer, idf, logisticRegression])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el pipeline, como un todo\n",
    "\n",
    "Recordemos que ahora mismo está activada la etapa `CountVectorizer`, con lo que ignoramos completamente el `HashingTF`. Nos servirá en el futuro cuando queramos decidir cuál de las dos opciones funciona mejor en base a su resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great</td>\n",
       "      <td>0.577814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>returned</td>\n",
       "      <td>-0.360140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poor</td>\n",
       "      <td>-0.335410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.312155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waste</td>\n",
       "      <td>-0.305736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>return</td>\n",
       "      <td>-0.289499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>useless</td>\n",
       "      <td>-0.284552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.263952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>-0.257197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>broke</td>\n",
       "      <td>-0.247735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>love</td>\n",
       "      <td>0.245701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>-0.242369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>junk</td>\n",
       "      <td>-0.231809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.231119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>returning</td>\n",
       "      <td>-0.230924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nice</td>\n",
       "      <td>0.220564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.219606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good</td>\n",
       "      <td>0.219273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>highly</td>\n",
       "      <td>0.218564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-0.216972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word    weight\n",
       "0           great  0.577814\n",
       "1        returned -0.360140\n",
       "2            poor -0.335410\n",
       "3         perfect  0.312155\n",
       "4           waste -0.305736\n",
       "5          return -0.289499\n",
       "6         useless -0.284552\n",
       "7            easy  0.263952\n",
       "8    disappointed -0.257197\n",
       "9           broke -0.247735\n",
       "10           love  0.245701\n",
       "11  disappointing -0.242369\n",
       "12           junk -0.231809\n",
       "13      excellent  0.231119\n",
       "14      returning -0.230924\n",
       "15           nice  0.220564\n",
       "16        awesome  0.219606\n",
       "17           good  0.219273\n",
       "18         highly  0.218564\n",
       "19       terrible -0.216972"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocabulary = pipelineModel.stages[2].vocabulary\n",
    "\n",
    "# De la lista de stages, nos quedamos con el último elemento (LogisticRegressionModel, ya entrenado: transformador)\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "\n",
    "# Obtenemos el array de coeficientes, que vienen en el mismo orden que las variables\n",
    "weights = lrModel.coefficients\n",
    "\n",
    "# Lista de pares (palabra, coeficiente) \n",
    "word_weight = list(zip(vocabulary,weights))\n",
    "\n",
    "word_weight.sort(key = lambda pair: np.abs(pair[1]), reverse = True)\n",
    "\n",
    "# Convertimos la lista de pares en un DataFrame para poder imprimirlo de manera más clara\n",
    "word_weight_df = pd.DataFrame(word_weight, columns = [\"word\", \"weight\"])[0:20]\n",
    "word_weight_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones en tiempo real con el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar Spark Structured Streaming para hacer predicciones. Aunque los datos originales tenían dos columnas separadas `summary` y `reviewText`, desde el principio habíamos concatenado ambas en una sola columna `reviewTS` que es la que se utiliza como punto de partida en el pipeline, que no necesita para nada `reviewText` ni `summary`. Por tanto nuestros datos en streaming tendrán una sola columna de tipo cadena de caracteres (string) llamada `reviewTS` que contiene en cada fila un texto completo (un string muy largo). \n",
    "\n",
    "Tampoco necesitamos ninguna columna de `label` ni valoración ni similar, puesto que son datos para predecir y asumimos que no tenemos por qué conocer dichos atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dar cada dato como si fuera un JSON completo en una sola línea. Después despiezamos cada JSON (cada línea) para pasarlo a un DataFrame de una columna de tipo string. Leermos cada JSON como una única línea de tipo string obtenida de Apache Kafka, configurando las siguientes opciones:\n",
    "\n",
    "  * Usamos la variable `readStream` (en lugar de `read` como solemos hacer) interna de la SparkSession `spark`\n",
    "  * Indicamos que el formato es `\"kafka\"` con `.format(\"kafka\")`\n",
    "  * Indicamos cuáles son los brokers de Kafka de los que vamos a leer y el puerto al que queremos conectarnos para leer (9092 es el que usa Kafka por defecto), con `.option(\"kafka.bootstrap.servers\", \"<nombre_cluster>-w-0:9092,<nombre_cluster>-w-1:9092\")`. De esa manera podremos leer el mensaje si el productor de Kafka lo envía a cualquiera de los dos brokers existentes, que son los nodos del cluster identificados como `<nombre_cluster>-w-0` y `<nombre_cluster>-w-1`\n",
    "  * Indicamos que queremos subscribirnos al topic `\"revisiones\"` con `.option(\"subscribe\", \"revisiones\")`.\n",
    "  * Finalmente ponemos `load()` para realizar la lectura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos (desde línea de comandos) el topic revisiones en Kafka\n",
    "`/usr/lib/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --replication-factor 1 --partitions 1 --topic revisiones`\n",
    "\n",
    "#### Vemos los topics existentes\n",
    "`/usr/lib/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --list`\n",
    "\n",
    "#### Abrimos el Kafka console producer (productor de Kafka desde consola, para enviarle mensajes al broker 0 del cluster de Kakfa)\n",
    "`/usr/lib/kafka/bin/kafka-console-producer.sh --broker-list <nombrecluster>-w-0:9092 --topic revisiones`\n",
    "\n",
    "#### Escribimos como mensajes: \n",
    "```\n",
    "{\"reviewTS\": \"This is an absolutely horrible product, what a shit!\"}\n",
    "{\"reviewTS\": \"The best purchase I have ever done, awesome\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos de Kafka suscribiéndonos al topic \"revisiones\" (revisiones de productos)\n",
    "textosStreamingDF = spark.readStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"ucmcluster-w-0:9092,ucmcluster-w-1:9092\")\\\n",
    "  .option(\"subscribe\", \"revisiones\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Este es el esquema de cada JSON: un único campo. \n",
    "# Spark lo parsea como una columna de tipo estructura que dentro tiene un único campo\n",
    "esquema = StructType([\\\n",
    "  StructField(\"reviewTS\", StringType())\\\n",
    "])\n",
    "\n",
    "parsedDF = textosStreamingDF\\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(StringType()))\\\n",
    "    .withColumn(\"reviewTS\", F.from_json(F.col(\"value\"), esquema))\\\n",
    "    .withColumn(\"reviewTS\", F.col(\"reviewTS.reviewTS\"))\n",
    "\n",
    "predictionsStreamingDF = pipelineModel.transform(parsedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predictionsStreamingDF\\\n",
    "                    .writeStream\\\n",
    "                    .queryName(\"predicciones\")\\\n",
    "                    .outputMode(\"append\")\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+--------+--------------+------------+---+--------+-------------+-----------+----------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|reviewTS|reviewTokensUf|reviewTokens| cv|features|rawPrediction|probability|prediction|\n",
      "+---+-----+-----+---------+------+---------+-------------+--------+--------------+------------+---+--------+-------------+-----------+----------+\n",
      "+---+-----+-----+---------+------+---------+-------------+--------+--------------+------------+---+--------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediccionesDF = spark.sql(\"select * from predicciones\")\n",
    "prediccionesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---------+------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| key|               value|     topic|partition|offset|           timestamp|timestampType|            reviewTS|      reviewTokensUf|        reviewTokens|                  cv|            features|       rawPrediction|         probability|prediction|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|null|{\"reviewTS\": \"Thi...|revisiones|        0|     1|2021-04-15 20:12:...|            0|This is an absolu...|[this, is, an, ab...|[absolutely, horr...|(16364,[8,688,961...|(16364,[8,688,961...|[0.72246589620740...|[0.67314979292700...|       0.0|\n",
      "|null|{\"reviewTS\": \"Thi...|revisiones|        0|     2|2021-04-15 20:12:...|            0|This is an absolu...|[this, is, an, ab...|[absolutely, horr...|(16364,[8,688,961...|(16364,[8,688,961...|[0.72246589620740...|[0.67314979292700...|       0.0|\n",
      "|null|{\"reviewTS\": \"Wha...|revisiones|        0|     3|2021-04-15 21:30:...|            0|What a wonderful day|[what, a, wonderf...|    [wonderful, day]|(16364,[99,1464],...|(16364,[99,1464],...|[-0.4544146591939...|[0.38831165539818...|       1.0|\n",
      "+----+--------------------+----------+---------+------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediccionesDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
