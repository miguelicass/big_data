{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Operaciones avanzadas con DataFrames"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> est\u00e1 compuesto por las siguientes variables referidas siempre al a\u00f1o 2018:\n\n1. **Month** 1-4\n2. **DayofMonth** 1-31\n3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n4. **FlightDate** fecha del vuelo\n5. **Origin** c\u00f3digo IATA del aeropuerto de origen\n6. **OriginCity** ciudad donde est\u00e1 el aeropuerto de origen\n7. **Dest** c\u00f3digo IATA del aeropuerto de destino\n8. **DestCity** ciudad donde est\u00e1 el aeropuerto de destino  \n9. **DepTime** hora real de salida (local, hhmm)\n10. **DepDelay** retraso a la salida, en minutos\n11. **ArrTime** hora real de llegada (local, hhmm)\n12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n13. **Cancelled** si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n14. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n15. **Diverted** si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n16. **ActualElapsedTime** tiempo real invertido en el vuelo\n17. **AirTime** en minutos\n18. **Distance** en millas\n19. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n20. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n22. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n23. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Leemos los datos y quitamos filas con NA y convertimos a num\u00e9ricas las columnas inferidas incorrectamente\nflightsDF = spark.read\\\n                 .option(\"header\", \"true\")\\\n                 .option(\"inferSchema\", \"true\")\\\n                 .csv(\"gs://ucmbucket/data/flights-jan-apr-2018.csv\")\n\n# Convertimos a enteros y re-categorizamos ArrDelay en una nueva columna ArrDelayCat\n# None (< 15 min), Slight(entre 15 y 60 min), Huge (> 60 min)\n\ncleanFlightsDF = flightsDF.withColumn(\"ArrDelayCat\", F.when(F.col(\"ArrDelay\") < 15, \"None\")\\\n                                                      .when((F.col(\"ArrDelay\") >= 15) & (F.col(\"ArrDelay\") < 60), \"Slight\")\\\n                                                      .otherwise(\"Huge\"))\\\n                           .cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Hagamos algunas preguntas a los datos para obtener conclusiones"}, {"cell_type": "markdown", "metadata": {}, "source": "Imaginemos que somos los due\u00f1os de una web de viajes que rastrea internet en busca de vuelos en agencias y otras p\u00e1ginas, los compara y recomienda el m\u00e1s adecuado para el aeropuerto. Junto con esta recomendaci\u00f3n, querr\u00edamos dar tambi\u00e9n informaci\u00f3n sobre vuelos fiables y no fiables en lo que respecta a la puntualidad. Esto depende de muchos factores, como el origen y destino, duraci\u00f3n del vuelo, hora del d\u00eda, etc."}, {"cell_type": "markdown", "metadata": {}, "source": "### Agrupaci\u00f3n y agregaciones"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<p><b>PREGUNTA</b>: \u00bfCu\u00e1les son los vuelos (origen, destino) con mayor retraso medio? \u00bfCu\u00e1ntos vuelos existen entre cada par de aeropuertos?</p>\n<p><b>PISTA</b>: Tras hacer las agregaciones para cada pareja \"Origin\", \"Dest\" (una agregaci\u00f3n para el retraso medio y otra para contar), aplica el m\u00e9todo sort(F.col(\"avgDelay\").desc()) para ordenar de forma decreciente por la nueva columna del retraso medio.\n</div>"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+------------------+-------+\n|Origin|Dest|          avgDelay|cuantos|\n+------+----+------------------+-------+\n|   RDM| MFR|            1347.0|      2|\n|   MDT| HPN|             798.0|      1|\n|   ORD| GTF|             212.0|      1|\n|   ICT| DAY|             210.0|      1|\n|   ELM| ATL|             169.0|      2|\n|   DSM| PIA|             168.0|      1|\n|   ERI| ITH|             160.0|      1|\n|   YNG| PIE|             141.0|      1|\n|   CMH| HOU|             120.0|      1|\n|   HRL| DAL|             111.0|      1|\n|   PPG| HNL|109.85714285714286|     35|\n|   HNL| PPG|105.85714285714286|     35|\n|   PIE| YNG|             104.0|      1|\n|   AVP| SFB|              93.0|      1|\n|   ACY| MSY| 87.45454545454545|     11|\n|   CPR| LAS|              85.0|      1|\n|   LAS| CPR|              82.0|      1|\n|   TTN| BNA|              76.5|     10|\n|   MSP| PVD|              74.0|      1|\n|   TUL| OKC|              69.0|      1|\n+------+----+------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql import functions as F\n\nretrasoMedioDF = flightsDF.groupBy(\"Origin\", \"Dest\").agg(F.mean(F.col(\"ArrDelay\")).alias(\"avgDelay\"),\n                                                         F.count(\"*\").alias(\"cuantos\"))\\\n                                                    .sort(F.col(\"avgDelay\").desc())\nretrasoMedioDF.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<p><b>PREGUNTA</b>: \u00bfEs el avi\u00f3n un medio de transporte fiable? Mostrar el n\u00famero de vuelos en cada categor\u00eda de retraso.</p>\nEn lugar de llamar agg(F.count(\"*\")), podemos llamar a la transformaci\u00f3n count() sobre el resultado de groupBy(), y crear\u00e1\nautom\u00e1ticamente una columna llamada \"count\" con los conteos para cada grupo.\n<p> Ahora agrupar tambi\u00e9n por cada aeropuerto de origen, y mostrando una columna distinta por cada tipo de retraso, con el recuento. PISTA: utilizar la funci\u00f3n pivot(\"colName\").</p>"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+\n|ArrDelayCat|  count|\n+-----------+-------+\n|     Slight| 298234|\n|       None|2004727|\n|       Huge| 200152|\n+-----------+-------+\n\n"}], "source": "cleanFlightsDF.groupBy(\"ArrDelayCat\").count().show()"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+-----------+-----+\n|Origin|Dest|ArrDelayCat|count|\n+------+----+-----------+-----+\n|   BOS| LGB|       Huge|   22|\n|   BOS| HOU|       None|  194|\n|   BUF| JFK|     Slight|  108|\n|   FLL| ORH|       None|   71|\n|   BOS| DEN|       Huge|   71|\n|   MSO| LAS|       None|   29|\n|   BIS| SFB|       None|   28|\n|   MSY| PIE|       None|   32|\n|   BOI| LAS|       None|  235|\n|   EWR| CVG|       None|  641|\n|   LEX| SFB|     Slight|   15|\n|   ROA| CLT|     Slight|  100|\n|   CLT| TLH|       None|  365|\n|   CLT| GRR|       None|  187|\n|   SFO| RDM|       None|  369|\n|   DEN| DFW|       None| 1578|\n|   COS| ORD|       Huge|   52|\n|   LAN| MSP|       None|  203|\n|   MSP| RAP|     Slight|   44|\n|   SBN| DTW|     Slight|   72|\n+------+----+-----------+-----+\nonly showing top 20 rows\n\n"}], "source": "cleanFlightsDF.groupBy(\"Origin\", \"Dest\", \"ArrDelayCat\").count().show()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+----+----+------+\n|Origin|Dest|Huge|None|Slight|\n+------+----+----+----+------+\n|   DSM| EWR|  10|  94|    14|\n|   STS| PHX|   9| 105|    14|\n|   PHL| MCO| 162|1291|   273|\n|   PBI| DCA|  37| 393|    49|\n|   ORD| PDX|  23| 528|    85|\n|   MCI| MKE|   8| 172|    18|\n|   SJC| LIH|   1|  83|     5|\n|   PBG| PGD|   1|  19|     6|\n|   BQN| MCO|   9|  85|    25|\n|   MDW| MEM|  22| 172|    42|\n|   FSD| ATL|   9|  83|     9|\n|   IAD| ILM|   2|  34|     8|\n|   MCI| IAH|  38| 487|    54|\n|   TPA| ACY|   4| 112|     4|\n|   SHD| LWB|   2|  25|  null|\n|   SMF| BUR|  61| 720|   124|\n|   ATL| GSP|  47|1080|   106|\n|   DSM| MCO|   1|  30|    10|\n|   SNA| PHX|  57| 967|   256|\n|   SPI| ORD|  50| 255|    37|\n+------+----+----+----+------+\nonly showing top 20 rows\n\n"}], "source": "cleanFlightsDF.groupBy(\"Origin\", \"Dest\").pivot(\"ArrDelayCat\").count().show()"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+-----------+------------+-----------+------------+-------------+--------------+\n|Origin|Dest|Huge_conteo|Huge_avgDist|None_conteo|None_avgDist|Slight_conteo|Slight_avgDist|\n+------+----+-----------+------------+-----------+------------+-------------+--------------+\n|   MCI| MKE|          8|       436.0|        172|       436.0|           18|         436.0|\n|   TPA| ACY|          4|       913.0|        112|       913.0|            4|         913.0|\n|   ORD| PDX|         23|      1739.0|        528|      1739.0|           85|        1739.0|\n|   MDW| MEM|         22|       480.0|        172|       480.0|           42|         480.0|\n|   SHD| LWB|          2|        87.0|         25|        87.0|         null|          null|\n|   STS| PHX|          9|       699.0|        105|       699.0|           14|         699.0|\n|   FSD| ATL|          9|       954.0|         83|       954.0|            9|         954.0|\n|   SMF| BUR|         61|       358.0|        720|       358.0|          124|         358.0|\n|   PBI| DCA|         37|       857.0|        393|       857.0|           49|         857.0|\n|   PHL| MCO|        162|       861.0|       1291|       861.0|          273|         861.0|\n|   DSM| EWR|         10|      1017.0|         94|      1017.0|           14|        1017.0|\n|   IAD| ILM|          2|       323.0|         34|       323.0|            8|         323.0|\n|   LBB| DEN|         20|       456.0|        184|       456.0|           20|         456.0|\n|   PIE| AVP|       null|        null|          1|      1006.0|         null|          null|\n|   ATL| GSP|         47|       153.0|       1080|       153.0|          106|         153.0|\n|   SNA| PHX|         57|       338.0|        967|       338.0|          256|         338.0|\n|   LAS| LIT|          5|      1294.0|         93|      1294.0|           22|        1294.0|\n|   DSM| MCO|          1|      1141.0|         30|      1141.0|           10|        1141.0|\n|   PBG| PGD|          1|      1311.0|         19|      1311.0|            6|        1311.0|\n|   BQN| MCO|          9|      1129.0|         85|      1129.0|           25|        1129.0|\n+------+----+-----------+------------+-----------+------------+-------------+--------------+\nonly showing top 20 rows\n\n"}], "source": "cleanFlightsDF.groupBy(\"Origin\", \"Dest\").pivot(\"ArrDelayCat\").agg(\n    F.count(\"*\").alias(\"conteo\"),\n    F.mean(\"Distance\").alias(\"avgDist\")\n).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<p><b>PREGUNTA</b>: \u00bfHay relaci\u00f3n entre el d\u00eda de la semana y el retraso a la salida o a la llegada?</p>\n    <p><b>PISTA</b>: Calcula el retraso medio a la salida y a la llegada para cada d\u00eda de la semana y ordena por una de ellas descendentemente.</p>\n    <p> Ahora haz lo mismo para cada d\u00eda pero solo con el retraso a la llegada, desagregado por cada aeropuerto de salida, utilizando la funci\u00f3n pivot(). </p>\n</div>"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+------------------+-------------------+\n|DayOfWeek|     avg(DepDelay)|      avg(ArrDelay)|\n+---------+------------------+-------------------+\n|        1|10.430177708665964|  5.391113068725289|\n|        5|10.220785437977693|  5.027363815430113|\n|        7| 9.142161259888235| 3.2344449424598207|\n|        3|  8.47071347600168| 3.0525338339576717|\n|        4|  8.35856546210902| 2.7390527404801026|\n|        2| 8.246502522185226| 2.8412409647873806|\n|        6| 6.278199328016013|-0.5748593305876211|\n+---------+------------------+-------------------+\n\n"}], "source": "avgDelaysDF = cleanFlightsDF.groupBy(\"DayOfWeek\").agg(F.mean(\"DepDelay\"), F.mean(\"ArrDelay\"))\\\n                            .orderBy(F.col(\"avg(DepDelay)\").desc())\navgDelaysDF.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<p><b>LA FUNCI\u00d3N PIVOT</b>: Puede ser interesante ver, para cada (Origin, Dest), el retraso promedio por\nd\u00eda de la semana. Si agrupamos por esas tres variables (Origin, Dest, DayOfWeek), nuestro resultado tendr\u00eda demasiadas filas para ser f\u00e1cil de visualizar (7 x 1009 ya que hay 1009 combinaciones de (Origin, DayOfWeek)). En cambio, vamos a crear 7 columnas, una por d\u00eda de la semana, en nuestro resultado DF. Lo haremos utilizando una de las variables de agrupaci\u00f3n (DayOfWeek) como <i> variable pivot</i>. Como esta variable tiene 7 valores distintos, se crear\u00e1n 7 columnas nuevas. De esta manera, visualizaremos toda la informaci\u00f3n de cada combinaci\u00f3n (Origen, Dest) condensada en una fila con 7 columnas con los 7 retrasos promedio correspondientes a ese (Origen, Dest) en cada d\u00eda de la semana.\n</div>"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+------+------+------+-----+------+------+------+\n|Origin|Dest|     1|     2|     3|    4|     5|     6|     7|\n+------+----+------+------+------+-----+------+------+------+\n|   ABE| ATL|  16.0| 17.12|  6.26|-1.38|  4.46|-12.15|  5.85|\n|   ABE| CLT|  0.05|  7.92|  5.18| 0.54| -0.36| 16.09| 12.89|\n|   ABE| DTW| 42.21| 19.29| 23.02|16.13| 40.39| -4.15|  9.37|\n|   ABE| FLL|  null|  null| 33.88| null|  null| 12.29|  null|\n|   ABE| MDT|  null|  null|  null| null|  null|  null|  null|\n|   ABE| MYR| -4.67|  -4.0|  null| null| 30.75|  null|  null|\n|   ABE| ORD| 17.97| 12.52| 33.69|26.06|   9.0|  9.47| 30.21|\n|   ABE| PGD| -0.22|  null|  0.73| null| 72.18|  null|  null|\n|   ABE| PHL| 10.44|  4.11| -1.44|-3.32|  0.51|  7.93|  3.03|\n|   ABE| PIE|  null|  17.8|  null|-3.59|  null|  -3.2|  2.78|\n|   ABE| SFB| -0.16| 27.71| 23.63|  2.0| 32.09| 21.46|  1.67|\n|   ABI| DFW| 10.65| 16.36| -0.55| 0.38|  4.64| 16.49|  4.61|\n|   ABQ| ATL|  2.28|-15.68|-11.15|-11.5| -6.29| -2.92|-14.38|\n|   ABQ| AUS|   3.6|  22.0|  30.0|-2.88| -4.62|  37.0| -3.98|\n|   ABQ| BWI|-10.09|-10.75|  -6.4| -4.6| -6.05| -8.55| -7.52|\n|   ABQ| DAL|  3.92|  8.93|   5.1| 3.58|  3.16|  0.52|  2.81|\n|   ABQ| DEN|  2.04| -1.22|   2.4| 0.65|   2.1| -1.98|  5.19|\n|   ABQ| DFW| -3.27|  4.25|  4.95|11.84| -2.06| -1.29| -3.29|\n|   ABQ| HOU|  8.83| -3.16|  3.32| 3.26| -4.29| -2.65| -3.81|\n|   ABQ| IAH| -9.08| -4.23|  3.51|-6.69|-13.55| -6.67| -8.92|\n+------+----+------+------+------+-----+------+------+------+\nonly showing top 20 rows\n\n"}], "source": "byOriginDestWithPivot = cleanFlightsDF.groupBy(\"Origin\", \"Dest\")\\\n                                 .pivot(\"DayOfWeek\")\\\n                                 .agg(F.round(F.mean(\"ArrDelay\"), 2))\\\n                                 .orderBy(\"Origin\", \"Dest\")\n\nbyOriginDestWithPivot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Operaciones JOIN"}, {"cell_type": "markdown", "metadata": {}, "source": "Ser\u00eda \u00fatil tener el retraso medio de una ruta junto a cada vuelo, para ver qu\u00e9 vuelos han tra\u00eddo un retraso mayor o menor que el retraso medio para esa ruta."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n    <b> PREGUNTA </b>:\nUsa el averageDelayOriginDestDF creado anteriormente, elimina la columna de conteo y luego \u00fanerlo con cleanFlightsDF, utilizando Origin y Dest como columnas de enlace. Finalmente, selecciona solo las columnas Origin, Dest, DayOfWeek, ArrDelay y avgDelay del resultado.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n    <p><b>BONUS (OPCIONAL)</b>: crear una nueva columna <i>belowAverage</i> que tenga valor True si ArrDelay es menor que el avgDelay de esa ruta, y False en caso contrario. No utilizar la funci\u00f3n when() sino el operador de comparaci\u00f3n directamente entre columnas, la cual devolver\u00e1 una columna booleana.\n</div>"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+--------+------------------+\n|Origin|Dest|ArrDelay|          avgDelay|\n+------+----+--------+------------------+\n|   ATL| GSP|   -12.0|-1.734910277324633|\n|   ATL| GSP|    -3.0|-1.734910277324633|\n|   ATL| GSP|    -6.0|-1.734910277324633|\n|   ATL| GSP|     6.0|-1.734910277324633|\n|   ATL| GSP|   -16.0|-1.734910277324633|\n|   ATL| GSP|    -2.0|-1.734910277324633|\n|   ATL| GSP|     4.0|-1.734910277324633|\n|   ATL| GSP|   -14.0|-1.734910277324633|\n|   ATL| GSP|   -17.0|-1.734910277324633|\n|   ATL| GSP|   -19.0|-1.734910277324633|\n|   ATL| GSP|    -9.0|-1.734910277324633|\n|   ATL| GSP|    -6.0|-1.734910277324633|\n|   ATL| GSP|   -14.0|-1.734910277324633|\n|   ATL| GSP|   -14.0|-1.734910277324633|\n|   ATL| GSP|   -22.0|-1.734910277324633|\n|   ATL| GSP|   -14.0|-1.734910277324633|\n|   ATL| GSP|   -19.0|-1.734910277324633|\n|   ATL| GSP|   -21.0|-1.734910277324633|\n|   ATL| GSP|   -24.0|-1.734910277324633|\n|   ATL| GSP|   -20.0|-1.734910277324633|\n+------+----+--------+------------------+\nonly showing top 20 rows\n\n"}], "source": "flightsWithAverage = cleanFlightsDF.join(retrasoMedioDF, on = [\"Origin\", \"Dest\"])\nflightsWithAverage.select(\"Origin\", \"Dest\", \"ArrDelay\", \"avgDelay\").show()"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+--------+-------------------+------------+\n|Origin|Dest|ArrDelay|           avgDelay|belowAverage|\n+------+----+--------+-------------------+------------+\n|   IAD| TPA|     -14|-10.310344827586206|        true|\n|   IAD| TPA|       2|-10.310344827586206|       false|\n|   IND| BWI|      14|0.49382716049382713|       false|\n|   IND| BWI|      -6|0.49382716049382713|        true|\n|   IND| BWI|      34|0.49382716049382713|       false|\n|   IND| JAX|      11|  7.862068965517241|       false|\n|   IND| LAS|      57|-2.2413793103448274|       false|\n|   IND| LAS|     -18|-2.2413793103448274|        true|\n|   IND| MCI|       2| 3.1296296296296298|        true|\n|   IND| MCI|     -16| 3.1296296296296298|        true|\n|   IND| MCO|       1|-0.5172413793103449|       false|\n|   IND| MCO|      80|-0.5172413793103449|       false|\n|   IND| MDW|       1| 11.514018691588785|        true|\n|   IND| MDW|      10| 11.514018691588785|        true|\n|   IND| MDW|      -4| 11.514018691588785|        true|\n|   IND| MDW|      11| 11.514018691588785|        true|\n|   IND| PHX|      15| -4.103448275862069|       false|\n|   IND| PHX|     -15| -4.103448275862069|        true|\n|   IND| TPA|      16| -9.172413793103448|       false|\n|   ISP| BWI|      37|  1.764102564102564|       false|\n+------+----+--------+-------------------+------------+\nonly showing top 20 rows\n\n"}], "source": "flightsComparedWithAverage = flightsWithAverage.withColumn(\"belowAverage\", F.col(\"ArrDelay\") < F.col(\"avgDelay\"))\n\nflightsComparedWithAverage.select(\"Origin\", \"Dest\", \"ArrDelay\", \"avgDelay\", \"belowAverage\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "**PREGUNTA**: repetir la operaci\u00f3n sin utilizar JOIN sino funciones de ventana"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+--------+------------------+------------+\n|Origin|Dest|ArrDelay|          avgDelay|belowAverage|\n+------+----+--------+------------------+------------+\n|   ATL| GSP|    -6.0|-1.734910277324633|        true|\n|   ATL| GSP|    -3.0|-1.734910277324633|        true|\n|   ATL| GSP|    47.0|-1.734910277324633|       false|\n|   ATL| GSP|    null|-1.734910277324633|        null|\n|   ATL| GSP|    -6.0|-1.734910277324633|        true|\n|   ATL| GSP|    38.0|-1.734910277324633|       false|\n|   ATL| GSP|   -13.0|-1.734910277324633|        true|\n|   ATL| GSP|   -15.0|-1.734910277324633|        true|\n|   ATL| GSP|    10.0|-1.734910277324633|       false|\n|   ATL| GSP|    -6.0|-1.734910277324633|        true|\n|   ATL| GSP|    -3.0|-1.734910277324633|        true|\n|   ATL| GSP|   -14.0|-1.734910277324633|        true|\n|   ATL| GSP|    10.0|-1.734910277324633|       false|\n|   ATL| GSP|    14.0|-1.734910277324633|       false|\n|   ATL| GSP|    -3.0|-1.734910277324633|        true|\n|   ATL| GSP|   -22.0|-1.734910277324633|        true|\n|   ATL| GSP|    -9.0|-1.734910277324633|        true|\n|   ATL| GSP|    -9.0|-1.734910277324633|        true|\n|   ATL| GSP|   -16.0|-1.734910277324633|        true|\n|   ATL| GSP|    25.0|-1.734910277324633|       false|\n+------+----+--------+------------------+------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql import Window\n\nw = Window().partitionBy(\"Origin\", \"Dest\")\n\n# Para obtener el mismo resultado que antes, adem\u00e1s de calcular el retraso medio, a\u00f1adimos tambi\u00e9n otra columna con \n# el n\u00famero de vuelos que hay en el dataset (es decir, entre enero y abril de 2018) para esa ruta. \nflightsWithAverageNoJoinDF = cleanFlightsDF.withColumn(\"avgDelay\", F.mean(\"ArrDelay\").over(w))\\\n                                           .withColumn(\"cuantos\", F.count(\"*\").over(w))\\\n                                           .withColumn(\"belowAverage\", F.col(\"ArrDelay\") < F.col(\"avgDelay\"))\n\nflightsWithAverageNoJoinDF.select(\"Origin\", \"Dest\", \"ArrDelay\", \"avgDelay\", \"belowAverage\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b> PREGUNTA </b>: Vamos a construir otro DF con informaci\u00f3n sobre los aeropuertos (en una situaci\u00f3n real, tendr\u00edamos otra tabla en la base de datos como la tabla de la entidad Aeropuerto). Sin embargo, solo tenemos informaci\u00f3n sobre algunos aeropuertos. Nos gustar\u00eda agregar esta informaci\u00f3n a cleanFlightsDF como nuevas columnas, teniendo en cuenta que queremos que la informaci\u00f3n del aeropuerto coincida con el aeropuerto de origen de flightsDF. Utilizar la operaci\u00f3n de uni\u00f3n adecuada para asegurarse de que no se perder\u00e1 ninguna de las filas existentes de cleanFlightsDF despu\u00e9s de la uni\u00f3n.\n</div>"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "airportsDF = spark.createDataFrame([\n    (\"JFK\", \"John F. Kennedy International Airport\", 1948),\n    (\"LIT\", \"Little Rock National Airport\", 1931),\n    (\"SEA\", \"Seattle-Tacoma International Airport\", 1949),\n], [\"IATA\", \"FullName\", \"Year\"])"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+------------------------------------+----+-----+----------+-------+-------+\n|Origin|FullName                            |Dest|Month|DayofMonth|DepTime|ArrTime|\n+------+------------------------------------+----+-----+----------+-------+-------+\n|SEA   |Seattle-Tacoma International Airport|JFK |1    |1         |2241   |741    |\n|SEA   |Seattle-Tacoma International Airport|LGB |1    |1         |1754   |2016   |\n|SEA   |Seattle-Tacoma International Airport|BOS |1    |1         |2351   |805    |\n|SEA   |Seattle-Tacoma International Airport|BOS |1    |1         |1437   |2302   |\n|SEA   |Seattle-Tacoma International Airport|LGB |1    |1         |1208   |1450   |\n|SEA   |Seattle-Tacoma International Airport|JFK |1    |2         |2254   |700    |\n|SEA   |Seattle-Tacoma International Airport|LGB |1    |2         |1759   |2032   |\n|SEA   |Seattle-Tacoma International Airport|BOS |1    |2         |9      |821    |\n|SEA   |Seattle-Tacoma International Airport|BOS |1    |2         |1328   |2129   |\n|SEA   |Seattle-Tacoma International Airport|LGB |1    |2         |1148   |1433   |\n+------+------------------------------------+----+-----+----------+-------+-------+\nonly showing top 10 rows\n\n"}], "source": "joinedFlightsDF = cleanFlightsDF.join(airportsDF, airportsDF.IATA == cleanFlightsDF.Origin, \"left_outer\")\n\n# PREGUNTA: mostrar algunas filas donde FullName no sea null\njoinedFlightsDF.where(\"FullName is not null\")\\\n               .select(\"Origin\", \"FullName\", \"Dest\", \"Month\", \"DayofMonth\", \"DepTime\", \"ArrTime\")\\\n               .show(10, False) # False para que no se corte la columna si su contenido es muy ancho (ejemplo: FullName)"}, {"cell_type": "markdown", "metadata": {}, "source": "## User-defined functions (UDFs)"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a construir un UDF para convertir millas a kil\u00f3metros. Ten en cuenta que esto podr\u00eda hacerse f\u00e1cilmente multiplicando directamente la columna de millas por 1.6 (y ser\u00eda mucho m\u00e1s eficiente), ya que Spark permite el producto entre una columna y un n\u00famero. En todos los casos en los que Spark proporciona funciones integradas para realizar una tarea (como esta), debes usar esas funciones y no una UDF. Las UDF deben emplearse solo cuando no hay otra opci\u00f3n.\n\nLa raz\u00f3n es que las funciones integradas de Spark est\u00e1n optimizadas y Catalyst, el optimizador autom\u00e1tico de c\u00f3digo integrado en Spark, puede optimizarlo a\u00fan m\u00e1s. Sin embargo, las UDF son una caja negra para Catalyst y su contenido no se optimizar\u00e1, y por lo tanto, generalmente son mucho m\u00e1s lentas."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "8.0\n+------+----+--------+------+\n|Origin|Dest|Distance|DistKM|\n+------+----+--------+------+\n|   PHL| DFW|  1303.0|2084.8|\n|   SEA| LAS|   867.0|1387.2|\n|   SLC| SAN|   626.0|1001.6|\n|   LAX| BWI|  2329.0|3726.4|\n|   RDU| ATL|   356.0| 569.6|\n+------+----+--------+------+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\n\n# Primer paso: crear una funci\u00f3n de Python que reciba UN n\u00famero y lo multiplique por 1.6\ndef milesToKm(miles):\n    return miles*1.6\n\n# Vamos a probarla\nprint(milesToKm(5)) # 5 millas a km: 8 km\n\n# Segundo paso: crear un objeto UDF que envuelva a nuestra funci\u00f3n. \n# Hay que especificar el tipo de dato que devuelve nuestra funci\u00f3n\nudfMilesToKm = F.udf(milesToKm, DoubleType())\n\n# Con esto, Spark ser\u00e1 capaz de llamar a nuestra funci\u00f3n milesToKm sobre cada uno de los valores de una columna num\u00e9rica.\n# Spark enviar\u00e1 el c\u00f3digo de nuestra funci\u00f3n a los executors a trav\u00e9s de la red, y cada executor la ejecutar\u00e1 sobre las\n# particiones (una por una) que est\u00e9n en ese executor\n\n# Tercer paso: vamos a probar la UDF a\u00f1adiendo una nueva columna con el resultado de la conversi\u00f3n\nflightsWithKm = cleanFlightsDF.withColumn(\"DistKm\", udfMilesToKm(F.col(\"Distance\")))\n\nflightsWithKm.select(\"Origin\", \"Dest\", \"Distance\", \"DistKM\")\\\n             .distinct()\\\n             .show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<p><b>BONUS</b>: Crea tu propia UDF que convierta DayOfWeek en una cadena.\nPuedes hacerlo creando una funci\u00f3n de Python que reciba un n\u00famero entero y devuelva el d\u00eda de la semana,\nsimplemente leyendo desde un vector de cadenas de longitud 7 el valor en la posici\u00f3n indicada por el argumento entero. Para la UDF, recuerda que tu funci\u00f3n devuelve un StringType(). Finalmente, prueba tu UDF creando una nueva columna \"DayOfWeekString\".\n</div>"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+---------+---------------+\n|Origin|Dest|DayOfWeek|DayOfWeekString|\n+------+----+---------+---------------+\n|   SEA| JFK|        2|        Tuesday|\n|   DFW| FLL|        7|         Sunday|\n|   PHL| SJU|        1|         Monday|\n|   GEG| DFW|        6|       Saturday|\n|   OAK| PHX|        2|        Tuesday|\n|   LAX| DEN|        7|         Sunday|\n|   LAX| SEA|        1|         Monday|\n|   OAK| PDX|        4|       Thursday|\n|   LAS| PDX|        5|         Friday|\n|   SLC| PDX|        5|         Friday|\n|   SJC| LIH|        6|       Saturday|\n|   SJC| EWR|        7|         Sunday|\n|   KOA| ANC|        7|         Sunday|\n|   SEA| DTW|        1|         Monday|\n|   DTW| SEA|        1|         Monday|\n|   SEA| BOS|        3|      Wednesday|\n|   JFK| LAS|        5|         Friday|\n|   ATL| XNA|        5|         Friday|\n|   OAK| SLC|        5|         Friday|\n|   MSP| ANC|        5|         Friday|\n+------+----+---------+---------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql.types import StringType\n\n# First step: we create a python function that turns an Integer into the day of the week as string\ndef dayOfWeekToString(dayInteger):\n    # In our data, Monday is 1 but python lists start at index 0\n    daysOfWeek = [\"\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    return daysOfWeek[dayInteger]\n\n# Second step: we wrap our function with a Spark UDF so that it can be called by Spark on every value of an entire column\n# This way Spark is able to send our function to the executors, which will eventually run the function on the partitions\n# of the data that each executor has\ndayOfWeekStringUDF = udf(dayOfWeekToString, StringType())\n\n# Third step: let's try out our UDF by appending a new column that results from transforming (through the UDF) the\n# existing column DayOfWeek\nflightsWithDayOfWeekStr = cleanFlightsDF.withColumn(\"DayOfWeekString\", dayOfWeekStringUDF(F.col(\"DayOfWeek\")))\n\nflightsWithDayOfWeekStr.select(\"Origin\", \"Dest\", \"DayOfWeek\", \"DayOfWeekString\")\\\n                       .distinct()\\\n                       .show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}