---
title: "Entregable de minería de datos y modelización predictiva"
author: "Miguel Ángel Castaño Ibáñez"
date: "1/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
En este entregable vamos a analizar los datos socio-económicos de las provincias españolas mediante un análisis clustering, reduciendo variables y agrupando las provincias más relacionadas.  
Cargamos las librerías:
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# files
library(readxl)
library(knitr)
# stadistica
library(pastecs)
library(lattice)
library(ggplot2)
library(corrplot)
#análisis componentes principades
library(factoextra)
library(FactoMineR)
# clúster
library(cluster) #Cluster
library(heatmaply)
library(NbClust)
# utills
library(readxl)
library(kableExtra)
```
A continuación, cargamos los datos
```{r}
provincias <- read_excel("Provincias.xlsx")
datos<- as.data.frame(provincias)
rownames(datos)<-datos[,1] 
datos<-datos[,-1]
```
  
Obtenemos un primer resumen de los datos y estadísticos descriptivos
```{r}
# summary
tabla_summary<-summary(datos)
knitr::kable(tabla_summary, caption = "Tabla resumen de las variables")

# descriptivos 
est <- stat.desc(datos, basic=FALSE)
knitr::kable(est, digits=2, caption="Estadísticos Descriptivos")
```

#### 1. Calcular la matriz de correlaciónes, y su representación gráfica ¿Cuáles son las variables más correlaciónadas de forma inversa?

```{r}
cor_matrix<-cor(datos, method="pearson")
knitr::kable(cor_matrix, digits =2,caption = "Correlaciones")
# gráfico
corrplot(cor_matrix, type="upper", order="hclust",tl.col="black", tl.srt=90)
```
  
Podemos apreciar como la mortalidad está fuertemente correlaccionada de forma inversa con la natalidad y la tasa de actividad de cada porvincia. Con una correlación inversa de manera más débil pero también importante podemos encontrar IPC (Índice de Precios al Consumidor) con la tasa de paro, la mortalidad con la tasa de paro. Finalmente, podemos concluir que en todas la provincias la tasa de mortalidad, tiene un índice de correlación inversa entre un 20% - 40% aproximado, con el resto de variables, exceptuando el CANE y el IPC.  
 

#### 2. Realizar un análisis de componentes principales sobre la matriz de correlaciónes, calculando 7 componentes. Estudiar los valores de los autovalores obtenidos y las gráficas que los resumen. ¿Cuál es el número adecuado de componentes?

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
fit <- PCA(datos,scale.unit=TRUE,ncp=7,graph=TRUE)
```
```{r}
#table autovalores
eig <- get_eigenvalue(fit)
knitr::kable(eig, digits =2,caption = "Autovalores")
```
  
Observando la tabla de autovalores considero hacer el análisis con 4 componentes, ya que estas serán suficientes para poder explicar hasta un 90% de la varianza total.
```{r}
#graphs
# fviz_eig(fit, geom="line")+ theme_grey()
fviz_eig(fit,addlabels=TRUE) #4 componentes
```
  
Además, esto se puede ver gráficamente donde podemos ver como a partir de las 4 y 5 componente la variabilidad disminuirá drásticamente.

#### 3. Hacer de nuevo el análisis sobre la matriz de correlaciónes pero ahora indicando el número de componentes principales que hemos decidido retener (Que expliquen aproximadamente el 90%). Sobre este análisis contestar los siguientes apartados.
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
fit <- PCA(datos,scale.unit=TRUE,ncp=4,graph=TRUE)
```
  
##### A) Mostrar los coeficientes para obtener las componentes principales ¿Cuál es la expresión para calcular la primera Componente en función de las variables originales?
```{r warning=FALSE}
knitr::kable(fit$svd$V, digits =3,caption = "Autovectores")
```
  
*CP1 = 0.294Poblacion - 0.106Mortalidad + 0.041Natalidad + 0.110IPC + 0.294NumEmpresas + 0.286Industria + 0.293Construccion + 0.293CTH + 0.282Infor + 0.292AFS + 0.291APT + 0.114TasaActividad - 0.014TasaParo + 0.294Ocupados + 0.291PIB + 0.018CANE + 0.292TVF + 0.172VS*
  
##### B) Mostar una tabla con las correlaciónes de las Variables con las Componentes Principales. Para cada Componente indicar las variables con las que está más correlaciónada
```{r}
#estadísticos asociados -> var
var<-get_pca_var(fit)
# coloreamos las mas correlacionadas
# no funciona he intentado usar el paquete kableExtra
# var_cor_color <- var$cor
# var_cor_color[1,1] <- cell_spec(var_cor_color[1,1], color = "red", bold = T)
# table
knitr::kable(var$cor, digits=3, caption="Correlaciones de la CP con las variables")
```
**- Componente 1.** Poblacion, NumEmpresas, Industria, Construccion, CTH, Infor, AFS, APT, Ocupados, PIB, TVF, VS.  
**- Componente 2.** Mortalidad en negativo, Natalidad, IPC (negativo), TasaActividad, TasaParo.  
**- Componente 3.** CANE.  
**- Componente 4.** Ninguna.  

##### C) Comentar los gráficos que representan las variables en los planos formados por las componentes, intentando explicar lo que representa cada componente

```{r}
#Representación gráfica variables
#1 y 2
fviz_pca_var(fit, axes = c(1,2), col.var="cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE )
```
  
Podemos observar como la Componente 1 representa: Poblacion, NumEmpresas, Industria, Construccion, CTH, Infor, AFS, APT, Ocupados, PIB, TVF.

La Componente 2 representa: Mortalidad en negativo, Natalidad, IPC en negativo, aunque debilmente, TasaActividad, aunque también con poca correlación, TasaParo.
```{r}
#1 y 3
fviz_pca_var(fit, axes = c(1,3), col.var="cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE )
```
  
La Componente 3 representa: CANE

```{r}
#1 y 4
fviz_pca_var(fit, axes = c(1,4), col.var="cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE )
```
  
La componente 4  representa: VS, además de estar representad en la Componente 1 casi por igual, es decir, esta variable se re presenta mejor en el plano con las componentes 1 y 4  

A continuación, podemos ver el resto de gráficos que repersentan las variables en los planos formados por las combinaciones de componenetes restante
```{r}
#2 y 3
fviz_pca_var(fit, axes = c(2,3), col.var="cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE )
#2 y 4
fviz_pca_var(fit, axes = c(2,4), col.var="cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE )
```
  
En este plano vemos como están mejor representada las variables IPC y TasaActividad, debido a su correlación mayor con estas componentes respectivamnete.
```{r}
#3 y 4
fviz_pca_var(fit, axes = c(3,4), col.var="cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),repel = TRUE )

```

##### D) Mostrar la tabla y los gráficos que nos muestran la proporción de la varianza de cada variable que es explicado por cada componente. ¿Cuál de las variables es la que está peor explicada?
```{r}
knitr::kable(var$cos2, digits =3,caption = "Cosenos al cuadrado")
#graph cosenos
corrplot(var$cos2,is.corr=FALSE)
# porcentaje  de variabilidad explicada por las 4 CP
fviz_cos2(fit,choice="var",axes=1:4)
```
  
Al menos 12 de nuestras 18 variables están explicadas más de un 90% y al menos la mitad con casi un 100%, el resto va disminuyendo hasta alcanzar valores próximos 75%. Estas variables menos representadas, como podemos ver en la tabla son (de mayor a menor representación): TasaActividad, TasaParo, CANE, VS, Natalidad, IPC


##### E) Mostrar la tabla y los gráficos que nos muestran el porcentaje de la varianza de cada Componente que es debido a cada variable. ¿Que variables contribuyen más a cada Componente?
```{r}
knitr::kable(var$contrib, digits =2,caption = "Contribuciones")
corrplot(var$contrib,is.corr=FALSE)
```
  
Este gráfico es muy útil, ya que te permite ver en un golpe de vista todas las variables y su representación en cada componente, pero vamos a profundizar y mostrar un gráfico componente a componente.
```{r}
# componente 1
fviz_contrib(fit,choice="var",axes=1)
```
  
Podemos ver como para formar la componente 1 las variables que más contribuyen  son: Ocupados, NumEmpresas, Poblacion, Construccion, CTH, AFS, TVF, PIB, APT, Industria, Infor
```{r}
# componente 2
fviz_contrib(fit,choice="var",axes=2)
```
  
Podemos ver como para formar la componente 2 las variables que más contribuyen  son: Mortalidad, Natalidad, TasaParo. Por ultimo IPC, TasaActividad también contribuyen pero de manera más debil.
```{r}
# componente 3
fviz_contrib(fit,choice="var",axes=3)
```
  
Podemos ver como para formar la componente 3 las variables que más contribuyen  es el CANE destacando sobre el resto, y también, aunque en una menor representación, la TasaPAro, TotalActividad, VS, Natalidad, IPC


```{r}
# componente 4
fviz_contrib(fit,choice="var",axes=4)
```
  
Podemos ver como para formar la componente 4 las variables que más contribuyen  son: VS, TasaActividad, IPC, y en menor medida el CANE.

##### F) Sobre los gráficos que representan las observaciones en los nuevos ejes y el gráfico Biplot., teniendo en cuenta la posición de las provincias en el gráfico Comentar las provincias que tienen una posición más destacada en cada componente, en positivo o negativo, ¿Qué significa esto en términos socioeconómicos para estas provincias?
```{r}
# 1 y 2
# fviz_pca_ind(fit, axes = c(1, 2), col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
fviz_pca_ind(fit, axes = c(1, 2), gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
#Representación conjunta de los individuos y las variables en los planos de las CP
fviz_pca_biplot(fit,  axes = c(1, 2), repel=TRUE, col.var="#2E9FDF", col.ind="#696969") # Variables y Individuals 
```
  
Podemos ver como Madrid y Barcelonas en todos los gráficos, destaca por su separación respecto a las demás observaciones en la Componente 1 (esto ya nos sugiere que pueden pertenecer a un clúster independiente del resto). En terminos socioecononomidos podemos afirmar que destacan sobre el resto de provincias, ya que esta componente medirá la industrialización, la productividad, la tasa de empleo y e indefinitiva índices económicos, que nos muestra como estas dos provincias están más desarrolladas que el resto. Podemos destacar Valencia y Alicante, o las provincias de Cataluña que destacaran también con un desarrollo económico que las provincias donde destaca una población más repartida en zonas rurales como las dos Castillas, Andalucía, etc. que están representadas en el lado izquierdo de la gráfica

En la componente 2 se representa: en el aspecto social la natalidad, mortalidad (de forma inversa), donde, las provincias del norte están en las partes bajas de la tabla, mientras que las zonas más al sur estarán en la parte superior, por ejemplo provincias de Andalucía y las dos ciudades autónomas Ceuta y Melilla, es decir, podemos conluir que la natalidad en el sur será mayor. También en un aspecto económico esta componente representa la Tasa de paro, por lo que se puede ver que en estas zonas del sur habrá una tasa más grande de desempleo.

```{r}
# 1 y 3
fviz_pca_ind(fit, axes = c(1, 3), gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
#Representación conjunta de los individuos y las variables en los plan os de las CP
fviz_pca_biplot(fit,  axes = c(1, 3), repel=TRUE, col.var="#2E9FDF", col.ind="#696969") # Variables y Individuals 
```
  
Aquí destacarán las provincias donde predomine el trabajo en el campo, ya que se representará en mayor parte el CANE (Censo Agrario Número de Explotaciones) podemos observar con las provincias de Galicia, además de Jaén, Valencia, Alicante, con una tradición agrícola destacan sobre el resto. Sin embargo, zonas más industrializadas como País Vasco o Ceuta y Melilla con poco terreno están en la parte más baja del plan, es decir sin apenas explotaciones agrícolas.

```{r}
# 1 y 4
fviz_pca_ind(fit, axes = c(1, 4), gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
#Representación conjunta de los individuos y las variables en los plan os de las CP
fviz_pca_biplot(fit,  axes = c(1, 4), repel=TRUE, col.var="#2E9FDF", col.ind="#696969") # Variables y Individuals 
```
  
En este plano destacar las provincias con zonas costeras como Valencia y Alicante, o que contenga una población de un poder económico mayor, por ejemplo las provincias de Cataluña, ya que representa el VS. (Censo de Viviendas secundarias). Estas estarán en la parte alta de la gráfica, mientras que zonas de interior o de un con una población más con poder adquisitivo menor estarán en la parte inferior, como es el caso de Melilla o Ceuta.

##### G) Si tuviéramos que construir un índice que valore de forma conjunta el desarrollo económico de una provincia, como se podría construir utilizando una combinación lineal de todas las variables. ¿Cuál sería el valor de dicho índice en Madrid? ¿Cual sería su valor en Melilla?  
Creo que la componente 1 podría ser un claro índice de desarrollo económico, por lo tanto para representas el valor de Madrid y Melilla lo podemos obtener de la siguiente tabla
```{r}
ind<-get_pca_ind(fit)
knitr::kable(ind$coord, digits =3,caption = "Valores de los individuos en las Cp")
```
  
Por lo tanto el valor es: Madrid = 16.778  y Melilla = -2.218  

#### 4. Representar un mapa de calor de la matriz de datos, estandarizado y sin estandarizar para ver si se detectan inicialmente grupos de provincias.  

A continuación, vemos en un mapa interactivo las variables y observaciones agrupadas según sus parecidos en los datos
```{r}
# grupos de calor juntando las más parecidas
# ggheatmap(datos, seriate = "mean" )
# interactivo
heatmaply(datos, seriate = "mean", row_dend_left = TRUE, plot_method = "plotly")
```
  
En un primer momento vamos a tratar los datos sin estandarizar.  
Procedemos a calcular la matriz de distancia entre las observaciones, y y mostramos las 6 primeras filas y columnas de esta.
```{r}
### Datos sin estandarizar
# matriz distancia, valores sin estandarizar 
dist <- dist(datos, method = "euclidean") # distance matrix 
# mostramos las 6 primeras filas
dist6<-as.matrix(dist)[1:6, 1:6]
knitr::kable(dist6, digits =2,caption = "Distancias")
```
  
Vemos el mapa de calor en los valores no estandarizados.
```{r}
# mapa de calor, valores sin estandarizar 
fviz_dist(dist, show_labels = TRUE)
```
  
En este primer mapa si que podemos ver claramente como Madrid y Barcelona están alejadas del resto de las observaciones. Valencia, Alicante, Bizkaia, Sevilla también tienen cierto parecido y podrían estar a priori en el mismo clúster. Desde Almería a Ciudad Real también un posible clúster destacado.  
En el mapa interactivo que se muestra a continuación se han reordenado las observaciones por proximidad, para visualizar mejor los posibles clúster.
```{r}
# reordenamos las observaciones
# ggheatmap(as.matrix(dist), seriate="mean")
# interactivo
heatmaply(as.matrix(dist), seriate = "OLO", row_dend_left = TRUE, plot_method = "plotly") 
# heatmaply(as.matrix(dist), seriate = "mean", row_dend_left = TRUE, plot_method = "plotly")
```
  
Ahora vamos a analizar estos datos estandarizados, para impedir que estas distancias dependan mucho de las escalas que se toman. De esta manera, conseguiremos tener un criterio de datos más homogeneizado.  

Procederemos a calcular la matriz de distancia con los datos estandarizados y observaremos las 6 primeras filas y columnas.
```{r}
### Datos estandarizados
datos_ST <- scale(datos)

# matriz distancia con, valores estandarizados
dist_st <- dist(datos_ST, method = "euclidean") # distance matrix
# mostramos las 6 primeras filas
dist6_st<-as.matrix(dist_st)[1:6, 1:6]
knitr::kable(dist6_st, digits =2,caption = "Distancia")
```
  
En este gráfico vemos el mapa de calor en los valores estandarizados.
```{r}
# mapa de calor, valores estandarizados
fviz_dist(dist_st)
```
  
En este mapa si que podemos ver de nuevo como Madrid y Barcelona están alejadas del resto de las observaciones. También podemos ver como provincias Valencia y Alicante, por otro lado Ceuta y Melilla tienen cierto parecido y podrían estar a priori en el mismo clúster. Desde Palencia hasta Valladolid también un posible clúster, aunque menos destacado.  

En el mapa interactivo que se muestra a continuación vamos a reordenar de nuevo las observaciones por proximidad y poder visualizar mejor los posibles clúster.
```{r}
# reordenamos las observaciones
# ggheatmap(as.matrix(dist_st), seriate="mean")
# interactivo
heatmaply(as.matrix(dist_st), seriate = "OLO", row_dend_left = TRUE, plot_method = "plotly") 
# heatmaply(as.matrix(dist_st), seriate = "mean", row_dend_left = TRUE, plot_method = "plotly")
```
#### 5. Realizar un análisis Jerárquico de clusters para determinar si existen grupos de provincias con comportamiento similar.  
Empezamos este análisis jerárquico de clusters.  

##### A) A la vista del dendrograma ¿Cuántos clusters recomendarías?  
Este dendrograma es el resultante de lo datos sin estandarizar
```{r}
# dendograma y agrupamos las observaciones, valores sin estandarizar 
res.hc <- hclust(dist, method="ward.D2")
fviz_dend(res.hc, cex = 0.5)
```
  
Aquí podemos observar 3 clusters a priori donde Madrid y Barcelona están en uno aparte como ya habíamos previsto en los mapas de calor.  

Este dendrograma es el resultante de lo datos estandarizados.
```{r}
# dendograma y agrupamos las observaciones, valores estandarizados
res.hc_st <- hclust(dist_st, method="ward.D2")
fviz_dend(res.hc_st, cex = 0.5)
```
  
Aquí podemos observar entre 4 y 6 clusters, donde sigue destacando Madrid y Barcelona como ya comentabamos anteriormente



##### B) Representar los individuos agrupados según el número de clusters elegido.  
He elegido 6 clúster ya que me parecían 6 los grupos más destacados en el dendograma de datos estandarizados. También observo las 10 primeras observaciones a que grupo pertenecen
```{r}
# agrupar en 6 clúster
grp <- cutree(res.hc_st, k = 6)
head(grp, n = 10)
knitr::kable(table(grp), caption = "Número de individuos por clúster")
```
  
A continuación, observo el dendograma con los clúster ya separados
```{r}
# visualizacion en el dendograma
# rownames(datos)[grp == 1]
fviz_dend(res.hc_st, k = 6, # Cut in four groups cex = 0.5, # label size
          k_colors = c("#ff0000", "#ffbb00", "#ffff00", "#00ff00", "#00ffff", "#0800ff"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE) # Add rectangle around groups
```
  
Y posteriormente en un mapa dimensional colas componentes 1 y 2
```{r}
# visualizacion
fviz_cluster(list(data = datos_ST, cluster = grp), 
             palette = c("#ff0000", "#ffbb00", "#ffff00", "#00ff00", "#00ffff", "#0800ff"), 
             ellipse.type = "convex", repel = TRUE, 
             show.clust.cent = FALSE, ggtheme = theme_minimal())
```

##### C) ¿Qué número óptimo de clusters nos indican los criterios Silhoutte y de Elbow?
```{r}
# Elbow: número optimo de clusters
fviz_nbclust(datos_ST, kmeans, method = "wss") + 
  geom_vline(xintercept =5, linetype = 2)+ labs(subtitle = "Elbow method")
```
  
El número optimo de clúster según el criterio de Elbow es 5
```{r}
# Silhouette: número optimo de clusters
fviz_nbclust(datos_ST, kmeans, method = "silhouette") + 
  labs(subtitle = "Silhouette method")
```
  
El número optimo de clúster según el metodo de Silhouette es 2  

##### D) Con el número de clusters decidido en el apartado anterior realizar un agrupamiento no jerárquico.  

Creamos la semilla de aleatoriedad fijada para obtener los mismos agrupamnientos.  
*En mac no siempre estará fijada esta semilla, por ello tras aplicar distintas veces esta semilla salen resultados diferentes, por lo tanto es posible que los comentarios no se correspondan con la gráfica resultante tras ejecutar la creacion del fichero html.*
```{r}
# misma semilla
RNGkind(sample.kind = "Rejection")
set.seed(1234)
```
  
Realizamos el agrupamiento para el análisis no jerárquico
```{r}
# No jerárquico
km.res <- kmeans(datos_ST, 5)
```
  
###### i- Representar los clusters formados en los planos de las Componentes principales. Relacionar la posición de cada clúster en el plano con lo que representa cada componente principal.
```{r}
fviz_cluster(km.res, datos_ST, axes = c(1, 2))
fviz_cluster(km.res, datos_ST, axes = c(1, 3))
fviz_cluster(km.res, datos_ST, axes = c(2, 4))
```
  
Aquí podemos el resultado de este agrupamiento mostrado en diferentes planos para mostrar las componentes de la 1 a la 4  

###### ii- Evaluar la calidad de los clusters  
Evaluamos el clúster con el criterio de Elbow
```{r}
# Criterio Elbow
# calidad de los clusters
sil <- silhouette(km.res$cluster, dist(datos_ST))
rownames(sil) <- rownames(datos)
head(sil[, 1:3])
# gráfica
fviz_silhouette(sil)
```
  
En la tabla podemos ver las primeras observaciones donde queda la distancia media entre los clúster vecinos. Por otro lado en el gráfico vemos una representación más visual de esta distancia a los clúster vecinos con todas las observaciones, donde podemos contemplar pocos valores en negativos. Esto nos indica que los clúster están bien definidos, es decir las observaciones están situadas en el clúster correcto.  
Por último voy a representar y posteriormente evaluar los clúster seleccionados con el método de Silhouette
```{r}
# Metodo Silhouette
km.res2 <- kmeans(datos_ST, 2)
fviz_cluster(km.res2, datos_ST, axes = c(1, 2))
```
  
Vemos como todas las observaciones quedan agrupadas en un solo clúster y por otro lado Madrid y Barcelona formando un clúster minoritario. Creo que utilizar este criterio para estas observaciones es malo, ya que conlleva a una falta de informacion debido a clasificar casi todas las observaciones en un clúster.
```{r}
# calidad de los clusters
sil2 <- silhouette(km.res2$cluster, dist(datos_ST))
rownames(sil2) <- rownames(datos)
head(sil2[, 1:3])
# gráfica
fviz_silhouette(sil2)
```
  
Sin embargo, en esta gráfica si podemos apreciar que la distancia al clúster vecino es bastante alta y no presenta ningún valor negativo, si no valores bastante elevados e igualados.


##### E) Explicar las provincias que forman cada uno de los clusters y comentar cuales son las características socioeconómicas que las hacen pertenecer a dicho clúster.  
En la tabla siguiente podemos ver todas las observaciones y al clúster perteneciente
```{r}
# grupo tabla
ordenado<-sort(km.res$cluster)
knitr::kable(ordenado, digits =2, caption = "Provincia y clúster")
```
  
En la siguiente tabla podemos ver las estadísticas de cada clúster con los datos estandarizados
```{r}
# estadísticas clúster, datos estandarizados 
knitr::kable(km.res$centers, digits =2,caption = "Estadísticos de los clusters, datos STD")
```
  
En la siguiente tabla podemos ver las estadísticas de cada clúster con los datos sin estandarizar
```{r}
# estadísticas clúster, datos originales estandarizados 
est_clust_orig<-aggregate(datos, by=list(km.res$cluster),mean)
knitr::kable(est_clust_orig, digits =2,caption = "Estadísticos de los clusters")
```
  
Vamos a definir las características de cada clúster, utilizando el criterio de Elbow (que yo considero el idóneo para la clasificación de estas observaciones, es decir 5 clúster).  
**- Cluster 1.** En este clúster podemos observar una mayoría de provincias del norte de la península, generalmente zonas más ricas e industrializadas.  
**- Cluster 2.** En este clúster se encuentran zonas rurales, enfocadas en el sector agrícola y generalmente más religiosas. Esto puede favorecer a una de la natalidad más alta, una mayor tasa de desempleo, como es el caso de estas provincias.  
**- Cluster 3.** En este tercer clúster también tiene unas características parecidas al clúster anterior pero con la diferencia de ser zonas costeras, turísticas, por ello puede conllevar a índices económicos mayores que las provincias clasificadas en el clúster 2.  
**- Cluster 4.** Por un lado contamos con el clúster 4 formado por Madrid y Barcelona destacando en índices económicos sobre el resto de provincias.  
**- Cluster 5.** En este clúster vemos sobre todo provincias del centro de la península, también enfocadas en un sector agrícola, población en zonas rurales, pero zonas con unos niveles en los índices económicos mayor que en el clúster 2.  


### Conclusión
Esta práctica creo que ha sido bastante útil para mi aprendizaje en el análisis de datos, ya que considero una parte fundamental de este campo y relacionado con posteriores modulos de machine learning y deep learning. Además también me ha gustado el dataset utilizado para esta practica, ya que ofrecía unos datos ya depurados e interesantes para analizar de una forma superficial la sociedad española.  




